"""
Answer Generator Agent - Á≠îÊ°àÁîüÊàêÂô®

Âü∫‰∫é Evidence Judge Âà§ÂÆö‰∏∫ sufficient ÁöÑ top 10 chunks ÁîüÊàêÊúÄÁªàÁ≠îÊ°à„ÄÇ

Ê†∏ÂøÉËÅåË¥£:
1. Êé•Êî∂ original question + top 10 high-quality chunks
2. ÁîüÊàêÁªìÊûÑÂåñÁöÑ„ÄÅÊúâËØÅÊçÆÊîØÊåÅÁöÑÁ≠îÊ°à
3. ÂºïÁî®ÂÖ∑‰ΩìÁöÑ chunk Êù•Ê∫ê
4. Á°Æ‰øùÁ≠îÊ°àÂáÜÁ°Æ„ÄÅÂÆåÊï¥„ÄÅÂèØËøΩÊ∫Ø

ËÆæËÆ°ÂéüÂàô:
- Á≠îÊ°àÂøÖÈ°ªÂü∫‰∫éÊèê‰æõÁöÑ chunksÔºà‰∏çËÉΩÂπªËßâÔºâ
- ÂøÖÈ°ªÂºïÁî®ÂÖ∑‰ΩìÁöÑ chunkÔºàÂèØËøΩÊ∫ØÊÄßÔºâ
- Â¶ÇÊûúËØÅÊçÆ‰∏çË∂≥Êüê‰∫õÊñπÈù¢ÔºåÊòéÁ°ÆËØ¥ÊòéÔºàlimitationsÔºâ
- ‰ª£Á†ÅÁÆÄÊ¥ÅÔºöprompts Âú® prompts/Ôºåformatting Âú® utils/
"""

from typing import Dict, Any
from pydantic import BaseModel, Field
from typing import Literal, List, Dict
from .base import BaseAgent
from ..state import AgenticRAGState
from ..prompts.answer_generator_prompts import (
    ANSWER_GENERATOR_SYSTEM_MESSAGE,
    build_answer_generation_prompt
)
from ..utils.chunk_formatting import (
    format_chunks_with_ids,
    format_cpt_descriptions
)


class Citation(BaseModel):
    """Single citation mapping citation number to chunk_id"""
    number: int = Field(description="Citation number used in answer, e.g., 1 for [1]")
    chunk_id: str = Field(description="Chunk ID being cited, e.g., 'chunk_000210'")


class CitedAnswer(BaseModel):
    """
    Answer Generator ÁöÑËæìÂá∫ÁªìÊûÑ
    
    ÂåÖÂê´Á≠îÊ°àÊñáÊú¨ÂíåËØÅÊçÆÂºïÁî®Ôºà‰ΩøÁî®Êï∞Â≠óÂºïÁî®Ê†ºÂºè [1] [2] [3]Ôºâ
    
    Ê≥®ÊÑèÔºöcitation_map ‰∏çÂú®Ê≠§Ê®°Âûã‰∏≠Ôºå‰ºöÂú®process()‰∏≠Ëá™Âä®ÁîüÊàê
    """
    answer: str = Field(
        description="Comprehensive answer with inline numbered citations [1] [2] [3]. MUST include citations after each claim."
    )
    key_points: List[str] = Field(
        default_factory=list,
        description="Key points with numbered citations [1] [2], e.g., 'Modifier 59 allowed [2] [3]'"
    )
    citations: List[Citation] = Field(
        default_factory=list,
        description="List of citations mapping citation numbers to chunk IDs. E.g., [{number: 1, chunk_id: 'chunk_000210'}, {number: 2, chunk_id: 'chunk_000345'}]"
    )
    confidence: float = Field(
        ge=0.0,
        le=1.0,
        description="Confidence score based on evidence quality (0.0-1.0)"
    )
    limitations: List[str] = Field(
        default_factory=list,
        description="Any limitations or caveats in the answer"
    )


class AnswerGeneratorAgent(BaseAgent):
    """
    Answer Generator Agent - ÁîüÊàêÊúÄÁªàÁ≠îÊ°à
    
    Â∑•‰ΩúÊµÅÁ®ã:
    1. Êé•Êî∂ original question Âíå top 10 chunksÔºàÂ∑≤Ë¢´ Evidence Judge È™åËØÅ‰∏∫ sufficientÔºâ
    2. ‰ΩøÁî® prompts/answer_generator_prompts.py ‰∏≠ÁöÑ prompt
    3. ËøîÂõûÁªìÊûÑÂåñÁ≠îÊ°àÔºàÂåÖÂê´ citations, key_points, confidenceÔºâ
    
    Token ‰ºòÂåñÁ≠ñÁï•:
    - Âè™Êé•Êî∂Â∑≤È™åËØÅ‰∏∫ sufficient ÁöÑ top 10 chunksÔºàÈÄöËøá conditional edgeÔºâ
    - ‰∏çÈáçÂ§çÂ±ïÁ§∫ evidence_assessmentÔºàÂ∑≤Âú® Evidence Judge ÂÆåÊàêÔºâ
    - CPT descriptions ÂçïÁã¨Ê†ºÂºèÂåñÔºåÈÅøÂÖçÂÜó‰Ωô
    """
    
    def __init__(self, config, client=None):
        """
        Args:
            config: Configuration object with Azure OpenAI settings
            client: Azure OpenAI client (optional, will use config.client if not provided)
        """
        self.config = config
        self._client = client if client is not None else getattr(config, 'client', None)
    
    @property
    def client(self):
        """Lazy initialization of LLM client"""
        if self._client is None:
            from openai import AzureOpenAI
            self._client = AzureOpenAI(
                api_key=self.config.azure_openai_api_key,
                api_version=self.config.azure_api_version,
                azure_endpoint=self.config.azure_openai_endpoint
            )
        return self._client
    
    def process(self, state: AgenticRAGState) -> dict:
        """
        ÁîüÊàêÂü∫‰∫éËØÅÊçÆÁöÑÁ≠îÊ°à
        
        Ê≥®ÊÑè: Ê≠§ÊñπÊ≥ïÂè™Âú® evidence is_sufficient=True Êó∂Ë¢´Ë∞ÉÁî®ÔºàÈÄöËøá conditional edgeÔºâ
        Âõ†Ê≠§Êó†ÈúÄÂÜçÊ£ÄÊü• evidence_assessment ÁöÑË¥®ÈáèÂàÜÊï∞
        
        Args:
            state: Contains question, retrieved_chunks (top 10), cpt_descriptions
            
        Returns:
            dict: Contains final_answer
        """
        question = state["question"]
        chunks = state.get("retrieved_chunks", [])
        cpt_descriptions = state.get("cpt_descriptions", {})
        
        # ÂÆâÂÖ®Ê£ÄÊü•ÔºöÁ°Æ‰øùÊúâ chunks
        if not chunks:
            return {
                "final_answer": {
                    "answer": "Êó†Ê≥ïÁîüÊàêÁ≠îÊ°àÔºöÊ≤°ÊúâÊ£ÄÁ¥¢Âà∞Áõ∏ÂÖ≥ËØÅÊçÆ„ÄÇ",
                    "key_points": [],
                    "citations": [],
                    "confidence": 0.0,
                    "limitations": ["Êú™Ê£ÄÁ¥¢Âà∞‰ªª‰ΩïÁõ∏ÂÖ≥ÊñáÊ°£"]
                }
            }
        
        # ‰ΩøÁî® utils Ê†ºÂºèÂåñ chunks Âíå CPT descriptions
        chunks_text = format_chunks_with_ids(chunks)
        cpt_desc_text = format_cpt_descriptions(cpt_descriptions)
        
        # ‰ΩøÁî® prompts/ ‰∏≠ÁöÑ prompt builder
        prompt = build_answer_generation_prompt(
            question=question,
            chunks_text=chunks_text,
            cpt_descriptions_text=cpt_desc_text
        )
        
        # Ë∞ÉÁî® LLM ÁîüÊàêÁªìÊûÑÂåñÁ≠îÊ°à
        response = self.client.beta.chat.completions.parse(
            model=self.config.azure_deployment_name,
            messages=[
                {"role": "system", "content": ANSWER_GENERATOR_SYSTEM_MESSAGE},
                {"role": "user", "content": prompt}
            ],
            response_format=CitedAnswer,
            temperature=self.config.agent_temperature
        )
        
        answer = response.choices[0].message.parsed
        
        # Generate citation_map from LLM's explicit Citation objects
        # This ensures correct mapping between citation numbers and chunk IDs
        citation_map = {
            citation.number: citation.chunk_id 
            for citation in answer.citations
        }
        
        # ‚úÖ VALIDATION: Verify citation mapping integrity
        print(f"\nüîç Citation Mapping Validation:")
        print(f"   LLM returned {len(answer.citations)} citation objects")
        
        # Display each citation mapping for verification
        for citation in sorted(answer.citations, key=lambda c: c.number):
            print(f"   [{citation.number}] ‚Üí {citation.chunk_id}")
        
        # Check for duplicate citation numbers
        citation_numbers = [c.number for c in answer.citations]
        if len(citation_numbers) != len(set(citation_numbers)):
            duplicates = [n for n in citation_numbers if citation_numbers.count(n) > 1]
            print(f"   ‚ö†Ô∏è  WARNING: Duplicate citation numbers detected: {set(duplicates)}")
        else:
            print(f"   ‚úÖ All citation numbers are unique")
        
        # Verify citation_map matches LLM output
        print(f"   ‚úÖ Generated citation_map with {len(citation_map)} entries")
        
        return {
            "final_answer": {
                "answer": answer.answer,
                "key_points": answer.key_points,
                "citation_map": citation_map,  # Explicit mapping from LLM
                "confidence": answer.confidence,
                "limitations": answer.limitations
            }
        }

