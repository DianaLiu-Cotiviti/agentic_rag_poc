"""
Planning Retrieval Router - LLMç”Ÿæˆè®¡åˆ’ï¼ŒAgentæ‰§è¡Œ

ç‰¹ç‚¹ï¼š
- 1æ¬¡LLMè°ƒç”¨ï¼ˆç”Ÿæˆplanï¼‰
- AgentæŒ‰ç…§planç¡¬ç¼–ç æ‰§è¡Œ
- ä¸­ç­‰é€Ÿåº¦ï¼ˆ~2ç§’ï¼‰
- ä¸­ç­‰æˆæœ¬ï¼ˆ$0.01ï¼‰
- LLMæ— æ³•æ ¹æ®ä¸­é—´ç»“æœè°ƒæ•´ï¼ˆplanå·²å®šï¼‰
"""

from typing import List, Dict, Any
from pydantic import BaseModel, Field
from ..state import AgenticRAGState, RetrievalResult
from ..prompts.retrieval_router_prompts import (
    build_retrieval_router_prompt,
    RETRIEVAL_ROUTER_SYSTEM_MESSAGE
)


# Pydantic Schemas for Planning Mode
class PreFilteringConfig(BaseModel):
    """Pre-retrieval filtering configuration"""
    apply_range_routing: bool = Field(description="Whether to apply range routing pre-filter")
    range_filter_cpt_codes: List[str] = Field(default_factory=list, description="CPT codes for range filtering")
    range_filter_limit: int = Field(default=300, ge=100, le=1000, description="Max chunks from range routing")


class QueryStrategyMapping(BaseModel):
    """Maps a query candidate to its best retrieval strategy
    
    æ¯ä¸ªqueryæ ¹æ®å…¶ç‰¹ç‚¹é€‰æ‹©æœ€é€‚åˆçš„strategyï¼š
    - hybrid: BM25+Semantic+RRFèåˆï¼ˆé€‚åˆå¤æ‚queryï¼‰
    - bm25: çº¯å…³é”®è¯æ£€ç´¢ï¼ˆé€‚åˆCPT codesã€ç²¾ç¡®åŒ¹é…ï¼‰
    - semantic: çº¯è¯­ä¹‰æ£€ç´¢ï¼ˆé€‚åˆæ¦‚å¿µæ€§queryï¼‰
    - bm25_semantic: å…ˆBM25å†Semanticï¼Œä¸¤ç»„ç»“æœéƒ½ä¿ç•™ï¼ˆé€‚åˆéœ€è¦åŒé‡éªŒè¯çš„queryï¼‰
    """
    query_index: int = Field(description="Index in query_candidates list")
    strategy: str = Field(
        description="Which retrieval strategy: hybrid, bm25, semantic, or bm25_semantic"
    )
    reasoning: str = Field(description="Why this strategy for this query")


class RetrievalParameters(BaseModel):
    """Retrieval parameters for each strategy"""
    bm25_top_k: int = Field(default=20, description="Top-k for BM25 search")
    semantic_top_k: int = Field(default=20, description="Top-k for semantic search")
    hybrid_top_k: int = Field(default=20, description="Top-k for hybrid search")
    hybrid_bm25_weight: float = Field(default=0.5, ge=0, le=1, description="BM25 weight in hybrid")
    hybrid_semantic_weight: float = Field(default=0.5, ge=0, le=1, description="Semantic weight in hybrid")


class FusionParameters(BaseModel):
    """Result fusion configuration (for multi-query fusion only)"""
    boost_range_results: float = Field(
        default=2.0,  # â† æé«˜é»˜è®¤å€¼åˆ°2.0
        ge=1.0,       # â† æœ€å°1.0ï¼ˆä¸é™åˆ†ï¼‰
        le=5.0,       # â† æé«˜æœ€å¤§å€¼åˆ°5.0ï¼Œå…è®¸æ›´å¼ºçš„boost
        description="Boost factor for range-routed chunks (higher = prioritize pre-filtered results more)"
    )


class RetrievalRouterDecision(BaseModel):
    """LLMç”Ÿæˆçš„æ£€ç´¢æ‰§è¡Œè®¡åˆ’
    
    æ¯ä¸ªqueryæ ¹æ®ç‰¹ç‚¹é€‰æ‹©æœ€é€‚åˆçš„strategyï¼ˆ4ç§ï¼‰ï¼š
    1. hybrid: BM25+Semantic+RRFï¼ˆtoolså±‚èåˆï¼‰ - é€‚åˆå¤æ‚query
    2. bm25: çº¯å…³é”®è¯æ£€ç´¢ - é€‚åˆCPT codesã€ç²¾ç¡®åŒ¹é…
    3. semantic: çº¯è¯­ä¹‰æ£€ç´¢ - é€‚åˆæ¦‚å¿µæ€§query
    4. bm25_semantic: å…ˆBM25å†Semanticï¼Œä¿ç•™ä¸¤ç»„ç»“æœ - é€‚åˆéœ€è¦åŒé‡éªŒè¯
    
    æ‰€æœ‰queryçš„ç»“æœæœ€åç”¨_aggregate_and_rankèšåˆæ’åº
    """
    pre_filtering: PreFilteringConfig
    query_strategy_mapping: List[QueryStrategyMapping] = Field(
        min_length=1,
        description="Each query selects best strategy from: hybrid, bm25, semantic, bm25_semantic"
    )
    retrieval_parameters: RetrievalParameters
    fusion_parameters: FusionParameters
    reasoning: str = Field(min_length=50, max_length=500)


class PlanningRetrievalRouter:
    """
    Planningæ¨¡å¼ - LLMç”Ÿæˆè®¡åˆ’ï¼ŒAgentæ‰§è¡Œ
    
    æ‰§è¡Œæµç¨‹ï¼š
    1. LLMç”ŸæˆRetrievalRouterDecisionï¼ˆPydantic schemaï¼‰
    2. AgentæŒ‰ç…§planç¡¬ç¼–ç æ‰§è¡Œï¼š
       - Pre-filtering (range routing)
       - æ¯ä¸ªqueryé€‰æ‹©æœ€é€‚åˆçš„strategy (hybrid/bm25/semantic/bm25_semantic)
       - æ‰€æœ‰queryçš„ç»“æœç”¨_aggregate_and_rankèšåˆæ’åº
    3. è¿”å›Top-Kç»“æœ
    
    è®¾è®¡åŸåˆ™ï¼š
    - Toolså±‚è´Ÿè´£ï¼šå•queryæ£€ç´¢ + hybridå†…éƒ¨èåˆ(BM25+Semantic+RRF)
    - Planningå±‚è´Ÿè´£ï¼šå¤šqueryç»“æœèšåˆ (æ‰€æœ‰strategiesçš„ç»“æœä¸€èµ·æ’åº)
    - æ¯ä¸ªqueryç‹¬ç«‹é€‰æ‹©strategyï¼Œæ ¹æ®queryç‰¹ç‚¹ï¼ˆCPT codeç”¨bm25ï¼Œå¤æ‚queryç”¨hybridç­‰ï¼‰
    - bm25_semantic: ä¸€ä¸ªqueryåŒæ—¶ç”¨BM25å’ŒSemanticï¼Œä¿ç•™ä¸¤ç»„ç»“æœ
    """
    
    def __init__(self, config, tools, client=None):
        """
        Args:
            config: Configuration object with Azure OpenAI settings
            tools: RetrievalTools instance
            client: Azure OpenAI client (optional, will use config.client if not provided)
        """
        self.config = config
        self.tools = tools
        self.client = client if client is not None else getattr(config, 'client', None)
    
    def process(self, state: AgenticRAGState) -> dict:
        """
        LLMç”Ÿæˆè®¡åˆ’ï¼ŒAgentæ‰§è¡Œ
        
        Supports retry mode: uses refined_queries when retry_count > 0
        
        Args:
            state: Contains question, query_candidates/refined_queries, retry_count, keep_chunks
            
        Returns:
            dict: Contains retrieved_chunks and retrieval_metadata
        """
        question = state["question"]
        question_type = state.get("question_type", "general")
        retrieval_strategies = state.get("retrieval_strategies", ["hybrid"])
        
        # RETRY MODE: Check if this is a retry round
        retry_count = state.get("retry_count", 0)
        
        # Use refined_queries if in retry mode, otherwise use query_candidates
        if retry_count > 0 and state.get("refined_queries"):
            # Retry mode: use refined queries from Query Refiner
            query_candidates = state.get("refined_queries", [])
            print(f"\nğŸ”„ RETRY MODE (Round {retry_count}) - Planning with {len(query_candidates)} refined queries")
            
            # Extract retrieval hints from refined queries (Query Refiner's hints)
            retrieval_hints = [
                rq.get("retrieval_hint") 
                for rq in query_candidates 
                if isinstance(rq, dict) and rq.get("retrieval_hint")
            ]
            if retrieval_hints:
                print(f"   Using {len(retrieval_hints)} retrieval hints from Query Refiner")
        else:
            # Initial mode: use query candidates from Query Planner
            query_candidates = state.get("query_candidates", [])
            # Get retrieval hints from Query Planner (strategy-level recommendations)
            retrieval_hints = state.get("retrieval_hints", [])
        
        # Build prompt for LLM to generate plan
        prompt = build_retrieval_router_prompt(
            question=question,
            question_type=question_type,
            retrieval_strategies=retrieval_strategies,
            query_candidates=[
                {
                    "query": qc.get("query") if isinstance(qc, dict) else qc.query,
                    "query_type": qc.get("query_type") if isinstance(qc, dict) else qc.query_type,
                    "weight": qc.get("weight", 1.0) if isinstance(qc, dict) else qc.weight
                }
                for qc in query_candidates
            ],
            retrieval_hints=retrieval_hints
        )
        
        # Call LLM for execution plan (å”¯ä¸€çš„LLMè°ƒç”¨)
        response = self.client.beta.chat.completions.parse(
            model=self.config.azure_deployment_name,
            messages=[
                {"role": "system", "content": RETRIEVAL_ROUTER_SYSTEM_MESSAGE},
                {"role": "user", "content": prompt}
            ],
            response_format=RetrievalRouterDecision,
            temperature=self.config.agent_temperature
        )
        
        decision = response.choices[0].message.parsed
        
        # Execute based on LLM's plan (Agentç¡¬ç¼–ç æ‰§è¡Œ)
        return self._execute_plan(state, decision)
    
    def _execute_plan(self, state: AgenticRAGState, decision: RetrievalRouterDecision) -> dict:
        """
        AgentæŒ‰ç…§LLMçš„planæ‰§è¡Œæ£€ç´¢
        
        âš ï¸ è¿™é‡Œå®Œå…¨æ˜¯Agentçš„ç¡¬ç¼–ç é€»è¾‘ï¼Œæ— LLMå‚ä¸
        
        Args:
            state: Current state
            decision: LLMç”Ÿæˆçš„æ‰§è¡Œè®¡åˆ’
            
        Returns:
            dict: Retrieved chunks and metadata
        """
        all_results = []
        metadata = {
            "mode": "planning",
            "retry_count": state.get("retry_count", 0),  # å§‹ç»ˆåŒ…å« retry_count
            "plan_reasoning": decision.reasoning,
            "strategies_used": [],  # æ¯ä¸ªqueryç”¨çš„strategy (will be set in Step 2)
            "total_chunks_retrieved": 0,
            "boost_range_results": decision.fusion_parameters.boost_range_results
        }
        
        # Step 1: Pre-filtering (Agentæ‰§è¡Œ)
        range_chunk_ids = set()
        cpt_descriptions = {}  # Store CPT descriptions for query enhancement
        
        if decision.pre_filtering.apply_range_routing:
            # Convert codes to integers
            cpt_codes_list = [int(code) for code in decision.pre_filtering.range_filter_cpt_codes]
            
            # Batch get all CPT descriptions at once (more efficient)
            cpt_descriptions = self.tools.get_cpt_descriptions(cpt_codes_list)
            
            # Get range routing chunks for each code
            for code in cpt_codes_list:
                try:
                    chunk_ids = self.tools.range_routing(
                        code,
                        limit=decision.pre_filtering.range_filter_limit
                    )
                    range_chunk_ids.update(chunk_ids)
                except:
                    pass  # Skip invalid codes
            metadata["range_chunks_count"] = len(range_chunk_ids)
            metadata["cpt_descriptions_used"] = cpt_descriptions
        
        # Step 2: Execute retrieval - æ¯ä¸ªqueryç”¨å…¶æœ€é€‚åˆçš„strategy (Agentæ‰§è¡Œ)
        query_candidates = state.get("query_candidates", [])
        params = decision.retrieval_parameters
        strategies_used = []  # è®°å½•ä½¿ç”¨çš„strategies
        per_query_stats = []  # Track detailed execution info
        
        for mapping in decision.query_strategy_mapping:
            # è·å–å¯¹åº”çš„query
            query = query_candidates[mapping.query_index]
            query_text = query.get("query") if isinstance(query, dict) else query.query
            query_weight = query.get("weight", 1.0) if isinstance(query, dict) else query.weight
            query_guidance = query.get("guidance") if isinstance(query, dict) else (query.guidance if hasattr(query, 'guidance') else None)
            strategy = mapping.strategy
            
            # Enhance query with CPT descriptions if available
            if cpt_descriptions:
                desc_text = " ".join(cpt_descriptions.values())
                query_text = f"{query_text} [CPT Description: {desc_text}]"
            
            # Initialize stats for this query
            query_stats = {
                "query_index": mapping.query_index,
                "query_text": query_text[:80] + "..." if len(query_text) > 80 else query_text,
                "strategy": strategy,
                "weight": query_weight,
                "tools_called": [],
                "chunks_retrieved": 0
            }
            
            # æ ¹æ®æ¯ä¸ªqueryçš„strategyæ‰§è¡Œå¯¹åº”çš„æ£€ç´¢
            if strategy == "hybrid":
                # hybrid: BM25+Semantic+RRF (èåˆåœ¨toolså±‚)
                results = self.tools.hybrid_search(
                    query_text,
                    top_k=params.hybrid_top_k,
                    bm25_weight=params.hybrid_bm25_weight,
                    semantic_weight=params.hybrid_semantic_weight,
                    guidance=query_guidance
                )
                
                query_stats["tools_called"].append("hybrid_search")
                query_stats["chunks_retrieved"] = len(results)
                
                for r in results:
                    r.score *= query_weight
                    if r.chunk_id in range_chunk_ids:
                        r.score *= decision.fusion_parameters.boost_range_results
                
                all_results.extend(results)
                strategies_used.append(f"q{mapping.query_index}:hybrid")
            
            elif strategy == "bm25":
                # bm25: çº¯å…³é”®è¯æ£€ç´¢
                results = self.tools.bm25_search(query_text, top_k=params.bm25_top_k)
                
                query_stats["tools_called"].append("bm25_search")
                query_stats["chunks_retrieved"] = len(results)
                
                for r in results:
                    r.score *= query_weight
                    if r.chunk_id in range_chunk_ids:
                        r.score *= decision.fusion_parameters.boost_range_results
                
                all_results.extend(results)
                strategies_used.append(f"q{mapping.query_index}:bm25")
            
            elif strategy == "semantic":
                # semantic: çº¯è¯­ä¹‰æ£€ç´¢
                results = self.tools.semantic_search(
                    query_text, 
                    top_k=params.semantic_top_k,
                    guidance=query_guidance
                )
                
                query_stats["tools_called"].append("semantic_search")
                query_stats["chunks_retrieved"] = len(results)
                
                for r in results:
                    r.score *= query_weight
                    if r.chunk_id in range_chunk_ids:
                        r.score *= decision.fusion_parameters.boost_range_results
                
                all_results.extend(results)
                strategies_used.append(f"q{mapping.query_index}:semantic")
            
            elif strategy == "bm25_semantic":
                # bm25_semantic: ä¸€ä¸ªqueryåŒæ—¶ç”¨BM25å’ŒSemanticï¼Œä¿ç•™ä¸¤ç»„ç»“æœ
                # BM25 retrieval
                bm25_results = self.tools.bm25_search(query_text, top_k=params.bm25_top_k)
                for r in bm25_results:
                    r.score *= query_weight
                    if r.chunk_id in range_chunk_ids:
                        r.score *= decision.fusion_parameters.boost_range_results
                all_results.extend(bm25_results)
                
                # Semantic retrieval
                semantic_results = self.tools.semantic_search(
                    query_text, 
                    top_k=params.semantic_top_k,
                    guidance=query_guidance
                )
                for r in semantic_results:
                    r.score *= query_weight
                    if r.chunk_id in range_chunk_ids:
                        r.score *= decision.fusion_parameters.boost_range_results
                all_results.extend(semantic_results)
                
                query_stats["tools_called"] = ["bm25_search", "semantic_search"]
                query_stats["chunks_retrieved"] = len(bm25_results) + len(semantic_results)
                
                strategies_used.append(f"q{mapping.query_index}:bm25_semantic")
            
            per_query_stats.append(query_stats)
        
        metadata["strategies_used"] = strategies_used  # è®°å½•æ¯ä¸ªqueryç”¨çš„strategy
        metadata["num_queries_executed"] = len(query_candidates)
        metadata["per_query_stats"] = per_query_stats  # Detailed execution info per query
        
        # Step 3: FusionæŒ‰ç…§LLMçš„ç­–ç•¥ (Agentæ‰§è¡Œ)
        # ç®€åŒ–ï¼šç›´æ¥èšåˆscoresï¼ˆå·²ç»åœ¨Step 2ä¸­åº”ç”¨äº†weightså’Œboostï¼‰
        aggregated_results = self._aggregate_and_rank(all_results)
        
        # Layer 2: Limit to reasonable number for Layer 3 (avoid sending too many to cross-encoder)
        # Planning mode typically retrieves more per query, so we limit here
        max_for_layer3 = 20  # Send at most 20 chunks to Layer 3 cross-encoder
        if len(aggregated_results) > max_for_layer3:
            aggregated_results = aggregated_results[:max_for_layer3]
            print(f"\nğŸ“Š Layer 2: Truncated {len(all_results)} â†’ {len(aggregated_results)} chunks for Layer 3")
        
        # RETRY MODE: Merge chunks if retry_count > 0
        retry_count = state.get("retry_count", 0)
        keep_chunks = state.get("keep_chunks", [])
        missing_aspects = state.get("missing_aspects", [])
        
        if retry_count > 0 and keep_chunks:
            print(f"\nğŸ”€ Merging chunks (Planning Mode - Round {retry_count}):")
            print(f"   - Keep chunks (adaptive): {len(keep_chunks)}")
            print(f"   - New chunks: {len(aggregated_results)}")
            print(f"   - Missing aspects: {len(missing_aspects)}")
            
            # Call merge_chunks_in_retry tool
            # è¿”å› 15-20 merged chunksï¼Œç„¶åä¼ ç»™ Evidence Judge çš„ Layer 3 rerank
            merged_results, merge_stats = self.tools.merge_chunks_in_retry(
                keep_chunks=keep_chunks,
                new_chunks=aggregated_results,
                missing_aspects=missing_aspects,
                quality_threshold=0.75,
                top_k=20  # è¿”å›æœ€å¤š 20 chunksï¼ŒEvidence Judge ä¼š rerank åˆ° top 10
            )
            
            final_results = merged_results
            
            print(f"\nâœ… Merge complete (Adaptive Selection):")
            print(f"   - Final chunks for Evidence Judge: {len(final_results)}")
            print(f"   - Kept from old (adaptive): {merge_stats.get('kept_old_chunks', 0)}")
            print(f"   - Added from new: {merge_stats.get('added_new_chunks', 0)}")
            print(f"   - Removed duplicates: {merge_stats.get('duplicates_removed', 0)}")
            print(f"   - Boosted for missing aspects: {merge_stats.get('boosted_chunks', 0)}")
            print(f"   â†’ Evidence Judge will rerank to top 10")
            
            # Update metadata with merge info
            metadata.update(merge_stats)
        else:
            # Initial retrieval (no merge)
            final_results = aggregated_results
        
        metadata["total_chunks_retrieved"] = len(final_results)
        metadata["unique_chunks_before_fusion"] = len(set(r.chunk_id for r in all_results))
        metadata["total_results_before_aggregation"] = len(all_results)
        
        # Save retrieved chunks to output
        from ..utils.save_workflow_outputs import save_retrieved_chunks
        saved_path = save_retrieved_chunks(
            chunks=final_results,
            question=state.get('question', ''),
            output_dir=self.config.retrieval_output_dir,
            metadata=metadata
        )
        metadata["saved_to"] = saved_path
        
        return {
            "retrieved_chunks": final_results,
            "retrieval_metadata": metadata,
            "cpt_descriptions": cpt_descriptions if cpt_descriptions else None
        }
    
    def _aggregate_and_rank(self, results: List[RetrievalResult]) -> List[RetrievalResult]:
        """
        èšåˆå’Œæ’åºæ£€ç´¢ç»“æœï¼ˆPlanningå±‚çš„èåˆï¼‰
        
        å¤„ç†ä¸¤ç§æƒ…å†µï¼š
        1. Multi-query aggregation: å¤šä¸ªqueriesæ£€ç´¢åˆ°åŒä¸€chunk â†’ åˆ†æ•°ç´¯åŠ 
        2. BM25+Semantic fusion: bm25_semanticç­–ç•¥ä¸‹ï¼ŒBM25å’ŒSemanticæ£€ç´¢åˆ°åŒä¸€chunk â†’ åˆ†æ•°ç´¯åŠ 
        
        Scoreså·²ç»åœ¨Step 2ä¸­åº”ç”¨äº†ï¼š
        - query_weight (æ¥è‡ª Query Planner)
        - range_boost (æ¥è‡ª LLM decision)
        
        è¿™é‡Œåªéœ€è¦ï¼š
        1. å»é‡ï¼ˆåŒä¸€chunkå¯èƒ½è¢«å¤šä¸ªqueriesæˆ–å¤šä¸ªmethodsæ£€ç´¢åˆ°ï¼‰
        2. èšåˆscoresï¼ˆç´¯åŠ ï¼‰
        3. æ’åº
        
        Args:
            results: All retrieval results with weighted scores
            
        Returns:
            Deduplicated and ranked results
        """
        # Group by chunk_id and sum scores
        chunk_scores = {}
        chunk_data = {}
        
        for r in results:
            if r.chunk_id not in chunk_scores:
                chunk_scores[r.chunk_id] = 0.0
                chunk_data[r.chunk_id] = r
            
            # Sum scores (å·²ç»åŒ…å« query_weight å’Œ range_boost)
            chunk_scores[r.chunk_id] += r.score
        
        # Sort by aggregated score (descending)
        sorted_chunks = sorted(
            chunk_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # Convert back to RetrievalResult
        ranked_results = []
        for chunk_id, score in sorted_chunks:
            result = chunk_data[chunk_id]
            result.score = score  # Update with aggregated score
            ranked_results.append(result)
        
        return ranked_results
