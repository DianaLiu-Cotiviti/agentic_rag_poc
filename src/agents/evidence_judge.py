"""
Evidence Judge Agent - è¯æ®è¯„åˆ¤å®˜

è´Ÿè´£è¯„ä¼°æ£€ç´¢åˆ°çš„è¯æ®è´¨é‡ï¼Œåˆ¤æ–­ï¼š
1. è¯æ®æ˜¯å¦å……åˆ†å›ç­”é—®é¢˜ (is_sufficient)
2. è¯æ®è¦†ç›–åº¦ (coverage_score)
3. è¯æ®ç›¸å…³æ€§å’Œå‡†ç¡®æ€§ (specificity_score)
4. æ˜¯å¦å­˜åœ¨çŸ›ç›¾ä¿¡æ¯ (has_contradiction)
5. ç¼ºå¤±çš„æ–¹é¢ (missing_aspects)
"""

from typing import List
from pydantic import BaseModel, Field
from .base import BaseAgent
from ..state import AgenticRAGState, RetrievalResult
from ..prompts.evidence_judge_prompts import (
    EVIDENCE_JUDGE_SYSTEM_MESSAGE,
    build_evidence_judgment_prompt
)


class EvidenceJudgment(BaseModel):
    """
    Evidence Judgeçš„è¯„ä¼°ç»“æœ
    
    åˆ¤æ–­æ ‡å‡†ï¼š
    - is_sufficient: è¯æ®æ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜ï¼ˆç»¼åˆè€ƒè™‘æ•°é‡ã€è´¨é‡ã€è¦†ç›–åº¦ï¼‰
    - coverage_score: è¯æ®å¯¹é—®é¢˜å„æ–¹é¢çš„è¦†ç›–ç¨‹åº¦ï¼ˆ0.0-1.0ï¼‰
    - specificity_score: è¯æ®çš„ç‰¹å®šæ€§å’Œå‡†ç¡®æ€§ï¼ˆ0.0-1.0ï¼‰
    - has_contradiction: æ£€ç´¢ç»“æœä¸­æ˜¯å¦å­˜åœ¨çŸ›ç›¾ä¿¡æ¯
    - missing_aspects: é—®é¢˜ä¸­æœªè¢«è¦†ç›–çš„æ–¹é¢ï¼ˆç”¨äºæŒ‡å¯¼é‡è¯•ï¼‰
    - reasoning: è¯„ä¼°æ¨ç†è¿‡ç¨‹ï¼ˆè§£é‡Šä¸ºä»€ä¹ˆsufficient/insufficientï¼‰
    """
    is_sufficient: bool = Field(
        description="Whether the evidence is sufficient to answer the question"
    )
    coverage_score: float = Field(
        ge=0.0,
        le=1.0,
        description="How well the evidence covers different aspects of the question (0.0-1.0)"
    )
    specificity_score: float = Field(
        ge=0.0,
        le=1.0,
        description="How specific and accurate the evidence is (0.0-1.0)"
    )
    has_contradiction: bool = Field(
        description="Whether there are contradictory statements in the evidence"
    )
    missing_aspects: List[str] = Field(
        default_factory=list,
        description="Aspects of the question not covered by current evidence"
    )
    reasoning: str = Field(
        description="Detailed reasoning for the sufficiency judgment"
    )


class EvidenceJudgeAgent(BaseAgent):
    """
    Evidence Judge Agent - è¯„ä¼°æ£€ç´¢è¯æ®è´¨é‡
    
    æ ¸å¿ƒèŒè´£ï¼š
    1. åˆ¤æ–­è¯æ®æ˜¯å¦å……åˆ† (is_sufficient)
       - è€ƒè™‘é—®é¢˜ç±»å‹ï¼ˆç®€å•CPT lookup vs å¤æ‚billingè§„åˆ™ï¼‰
       - è€ƒè™‘è¯æ®æ•°é‡å’Œè´¨é‡
       - è€ƒè™‘è¦†ç›–åº¦
    
    2. è¯„ä¼°è¯æ®è´¨é‡æŒ‡æ ‡ï¼š
       - coverage_score: è¦†ç›–é—®é¢˜çš„å¤šä¸ªæ–¹é¢ï¼ˆCPT codeå®šä¹‰ã€modifierã€bundlingç­‰ï¼‰
       - specificity_score: è¯æ®çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§
    
    3. è¯†åˆ«é—®é¢˜ï¼š
       - has_contradiction: æ£€æµ‹çŸ›ç›¾ä¿¡æ¯
       - missing_aspects: è¯†åˆ«ç¼ºå¤±çš„æ–¹é¢
    
    4. æŒ‡å¯¼ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š
       - å¦‚æœinsufficientï¼Œmissing_aspectsæŒ‡å¯¼query refinement
       - å¦‚æœsufficientï¼Œé«˜è´¨é‡chunksç”¨äºanswer generation
    """
    
    def __init__(self, config, client=None):
        """
        Args:
            config: Configuration object with Azure OpenAI settings
            client: Azure OpenAI client (optional, will use config.client if not provided)
        """
        self.config = config
        self._client = client if client is not None else getattr(config, 'client', None)
    
    @property
    def client(self):
        """Lazy initialization of LLM client"""
        if self._client is None:
            from openai import AzureOpenAI
            self._client = AzureOpenAI(
                api_key=self.config.azure_openai_api_key,
                api_version=self.config.azure_api_version,
                azure_endpoint=self.config.azure_openai_endpoint
            )
        return self._client
    
    def process(self, state: AgenticRAGState) -> dict:
        """
        è¯„ä¼°æ£€ç´¢è¯æ®è´¨é‡
        
        Args:
            state: Contains question, question_type, retrieved_chunks, cpt_descriptions
            
        Returns:
            dict: Contains evidence_assessment
        """
        question = state["question"]
        question_type = state.get("question_type", "general")
        chunks = state.get("retrieved_chunks", [])
        retrieval_metadata = state.get("retrieval_metadata", {})
        cpt_descriptions = state.get("cpt_descriptions", {})  # Get CPT descriptions from state
        
        # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°å†…å®¹ - æ˜ç¡®insufficient
        if not chunks:
            return {
                "evidence_assessment": {
                    "is_sufficient": False,
                    "coverage_score": 0.0,
                    "specificity_score": 0.0,
                    "has_contradiction": False,
                    "missing_aspects": ["No chunks retrieved - all aspects missing"],
                    "reasoning": "No relevant chunks were retrieved. Need to refine query or adjust retrieval strategy."
                }
            }
        
        # Apply cross-encoder reranking if enabled
        reranked_chunks = chunks  # Keep original for comparison
        if self.config.use_cross_encoder_rerank and len(chunks) > self.config.cross_encoder_top_k:
            print(f"\nğŸ”„ Layer 3 Reranking: Cross-Encoder (Question-aware)")
            print(f"   Purpose: Refine {len(chunks)} chunks to top {self.config.cross_encoder_top_k} based on original question")
            print(f"   Before: {len(chunks)} chunks (from Layer 1-2 fusion)")
            reranked_chunks = self._cross_encoder_rerank(question, chunks)
            print(f"   After: {len(reranked_chunks)} chunks (optimized for Evidence Judge)")
            retrieval_metadata["cross_encoder_reranked"] = True
            retrieval_metadata["cross_encoder_model"] = self.config.cross_encoder_model
            retrieval_metadata["chunks_before_layer3"] = len(chunks)
            retrieval_metadata["chunks_after_layer3"] = len(reranked_chunks)
            
            # Save reranked chunks to file for analysis
            self._save_reranked_chunks(question, chunks, reranked_chunks, retrieval_metadata)
        else:
            if not self.config.use_cross_encoder_rerank:
                print(f"\nâ­ï¸  Layer 3 Reranking: Skipped (disabled in config)")
            else:
                print(f"\nâ­ï¸  Layer 3 Reranking: Skipped (only {len(chunks)} chunks, threshold is {self.config.cross_encoder_top_k})")

        
        # Use reranked chunks for evaluation
        chunks_to_judge = reranked_chunks
        
        # æ„å»ºpromptç”¨äºLLMè¯„ä¼°
        # æ³¨æ„ï¼šåªç”¨ original question å’Œ retrieved chunks è¯„ä¼°
        # ä¸éœ€è¦ sub-queriesï¼ˆå®ƒä»¬åªæ˜¯æ£€ç´¢æ‰‹æ®µï¼Œä¸æ˜¯è¯„ä¼°ç›®æ ‡ï¼‰
        prompt = self._build_judgment_prompt(
            question=question,
            question_type=question_type,
            chunks=chunks_to_judge,
            retrieval_metadata=retrieval_metadata,
            cpt_descriptions=cpt_descriptions  # Pass CPT descriptions to prompt builder
        )
        
        # è°ƒç”¨LLMè¿›è¡Œç»“æ„åŒ–è¯„ä¼°
        response = self.client.beta.chat.completions.parse(
            model=self.config.azure_deployment_name,
            messages=[
                {"role": "system", "content": EVIDENCE_JUDGE_SYSTEM_MESSAGE},
                {"role": "user", "content": prompt}
            ],
            response_format=EvidenceJudgment,
            temperature=self.config.agent_temperature
        )
        
        judgment = response.choices[0].message.parsed
        
        # Return updated state with reranked chunks
        return {
            "evidence_assessment": {
                "is_sufficient": judgment.is_sufficient,
                "coverage_score": judgment.coverage_score,
                "specificity_score": judgment.specificity_score,
                "has_contradiction": judgment.has_contradiction,
                "missing_aspects": judgment.missing_aspects,
                "reasoning": judgment.reasoning
            },
            "retrieved_chunks": reranked_chunks,  # Update state with top-10 reranked chunks
            "retrieval_metadata": retrieval_metadata  # Update metadata with Layer 3 info
        }
    
    def _build_judgment_prompt(
        self,
        question: str,
        question_type: str,
        chunks: List[RetrievalResult],
        retrieval_metadata: dict,
        cpt_descriptions: dict = None
    ) -> str:
        """
        æ„å»ºEvidence Judgeçš„è¯„ä¼°prompt
        
        è¯„ä¼°é€»è¾‘ï¼š
        - è¯„ä¼°ç›®æ ‡ï¼šoriginal questionï¼ˆä¸æ˜¯sub-queriesï¼‰
        - è¯„ä¼°è¯æ®ï¼šretrieved chunks + CPT descriptionsï¼ˆå·²èåˆï¼‰
        - è¯„ä¼°æ ‡å‡†ï¼šquestion_type å¯¹åº”çš„ required aspects
        
        Args:
            question: Original user questionï¼ˆè¯„ä¼°ç›®æ ‡ï¼‰
            question_type: Question type
            chunks: Retrieved chunksï¼ˆå·²èåˆçš„15-20ä¸ªchunksï¼‰
            retrieval_metadata: Retrieval metadata
            cpt_descriptions: CPT code -> description mapping (from retrieval)
        """
        # Format chunks
        chunks_text = self._format_chunks_for_evaluation(chunks)
        
        # Format CPT descriptions (if available)
        cpt_desc_text = ""
        if cpt_descriptions:
            cpt_desc_text = "\n\n### ğŸ“‹ CPT Code Definitions (Retrieved)\n\n"
            for code, description in cpt_descriptions.items():
                cpt_desc_text += f"**CPT {code}**: {description}\n\n"
        
        # Extract metadata
        retrieval_mode = retrieval_metadata.get("mode", "unknown")
        strategies_used = retrieval_metadata.get("strategies_used", "N/A")
        
        # Use centralized prompt builder
        return build_evidence_judgment_prompt(
            question=question,
            question_type=question_type,
            chunks_text=chunks_text + cpt_desc_text,  # Append CPT descriptions to chunks
            retrieval_mode=retrieval_mode,
            strategies_used=str(strategies_used),
            total_chunks=len(chunks)
        )
    
    def _format_chunks_for_evaluation(self, chunks: List[RetrievalResult]) -> str:
        """
        åˆ†å±‚å±•ç¤ºchunks + LLMæ€»ç»“æˆªæ–­éƒ¨åˆ†ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
        
        ç­–ç•¥ï¼ˆæ–¹æ¡ˆ1+3æ··åˆï¼‰ï¼š
        - Top 5: å‰800å­—ç¬¦ + LLMæ€»ç»“å‰©ä½™éƒ¨åˆ†ï¼ˆå¦‚æœè¢«æˆªæ–­ï¼‰
        - Chunk 6-10: å‰400å­—ç¬¦ + LLMæ€»ç»“å‰©ä½™éƒ¨åˆ†ï¼ˆå¦‚æœè¢«æˆªæ–­ï¼‰
        - Chunk 11-20: å‰200å­—ç¬¦ + LLMæ€»ç»“å‰©ä½™éƒ¨åˆ†ï¼ˆå¦‚æœè¢«æˆªæ–­ï¼‰
        - æ‰¹é‡è°ƒç”¨ï¼š1æ¬¡LLMè°ƒç”¨å¤„ç†æ‰€æœ‰æˆªæ–­éƒ¨åˆ†çš„æ€»ç»“
        
        Args:
            chunks: List of RetrievalResult objects
        """
        if not chunks:
            return "No chunks retrieved."
        
        # Step 1: æ”¶é›†æ‰€æœ‰éœ€è¦æ€»ç»“çš„æˆªæ–­éƒ¨åˆ†
        chunks_to_summarize = []
        for i, chunk in enumerate(chunks[:20]):
            chunk_id, score, text, metadata = self._extract_chunk_data(chunk)
            
            # æ ¹æ®tierå†³å®šå±•ç¤ºé•¿åº¦
            if i < 5:
                preview_len = 800
                tier = "detailed"
            elif i < 10:
                preview_len = 400
                tier = "medium"
            else:
                preview_len = 200
                tier = "brief"
            
            # å¦‚æœæ–‡æœ¬è¢«æˆªæ–­ï¼Œè®°å½•éœ€è¦æ€»ç»“çš„éƒ¨åˆ†
            if len(text) > preview_len:
                chunks_to_summarize.append({
                    "index": i,
                    "tier": tier,
                    "preview": text[:preview_len],
                    "remaining": text[preview_len:],  # è¢«æˆªæ–­çš„éƒ¨åˆ†
                    "chunk_id": chunk_id,
                    "score": score,
                    "metadata": metadata
                })
            else:
                chunks_to_summarize.append({
                    "index": i,
                    "tier": tier,
                    "preview": text,
                    "remaining": None,  # æ— éœ€æ€»ç»“
                    "chunk_id": chunk_id,
                    "score": score,
                    "metadata": metadata
                })
        
        # Step 2: æ‰¹é‡æ€»ç»“æ‰€æœ‰æˆªæ–­éƒ¨åˆ†ï¼ˆ1æ¬¡LLMè°ƒç”¨ï¼‰
        summaries = self._batch_summarize_truncated_parts(chunks_to_summarize)
        
        # Step 3: æ ¼å¼åŒ–å±•ç¤ºï¼ˆå°†æ€»ç»“ç›´æ¥appendåˆ°previewåé¢ï¼‰
        formatted_chunks = []
        
        for item in chunks_to_summarize:
            i = item["index"]
            tier = item["tier"]
            chunk_id = item["chunk_id"]
            score = item["score"]
            metadata = item["metadata"]
            preview = item["preview"]
            summary = summaries.get(i, "")  # è·å–è¯¥chunkçš„æ€»ç»“
            
            # å°†æ€»ç»“ç›´æ¥æ‹¼æ¥åˆ°previewåé¢ï¼Œå½¢æˆå®Œæ•´æ–‡æœ¬
            full_text = f"{preview} {summary}" if summary else preview
            
            cpt_info = f" [CPT: {metadata.get('cpt_code')}]" if metadata.get("cpt_code") else ""
            
            # æ ¹æ®tieræ ¼å¼åŒ–ï¼ˆæ€»ç»“å·²appendåˆ°full_textä¸­ï¼‰
            if tier == "detailed":
                formatted_chunks.append(
                    f"**[Detailed {i+1}]** (ID: {chunk_id}, Score: {score:.4f}){cpt_info}\n"
                    f"{full_text}"
                )
            elif tier == "medium":
                formatted_chunks.append(
                    f"**[Medium {i+1}]** (Score: {score:.4f}){cpt_info}\n"
                    f"{full_text}"
                )
            else:  # brief
                formatted_chunks.append(
                    f"**[Brief {i+1}]** (Score: {score:.4f}){cpt_info}\n"
                    f"{full_text}"
                )
        
        # Step 4: æ•´ä½“ç»Ÿè®¡ä¿¡æ¯
        summary_info = f"\n\n**ğŸ“Š Overall Summary**\n"
        summary_info += f"Total chunks analyzed: {len(chunks)}\n"
        summary_info += f"Score range: {chunks[0].score:.4f} (highest) to {chunks[min(len(chunks)-1, 19)].score:.4f} (lowest)"
        
        return "\n\n".join(formatted_chunks) + summary_info
    
    def _batch_summarize_truncated_parts(self, chunks_data: List[dict]) -> dict:
        """
        æ‰¹é‡æ€»ç»“æ‰€æœ‰è¢«æˆªæ–­çš„chunkéƒ¨åˆ†ï¼ˆ1æ¬¡LLMè°ƒç”¨ï¼‰
        
        ä½¿ç”¨é…ç½®çš„ä¸»æ¨¡å‹è¿›è¡Œæ€»ç»“
        
        Args:
            chunks_data: List of chunk data with 'remaining' text to summarize
            
        Returns:
            dict: {chunk_index: summary_text}
        """
        # æ”¶é›†éœ€è¦æ€»ç»“çš„chunks
        to_summarize = [
            (item["index"], item["tier"], item["remaining"])
            for item in chunks_data
            if item["remaining"]  # åªæ€»ç»“è¢«æˆªæ–­çš„éƒ¨åˆ†
        ]
        
        if not to_summarize:
            return {}  # æ— éœ€æ€»ç»“
        
        # æ„å»ºæ‰¹é‡æ€»ç»“prompt
        prompt = "Summarize the continuation of each chunk below. Be concise but capture key medical coding details.\n\n"
        
        for idx, tier, remaining_text in to_summarize:
            # æ ¹æ®tierå†³å®šæ€»ç»“é•¿åº¦å’Œè¦æ±‚
            if tier == "detailed":
                instruction = "Provide detailed summary (2-3 sentences covering key medical coding details)"
            elif tier == "medium":
                instruction = "Provide medium summary (1-2 sentences with main points)"
            else:  # brief
                instruction = "Provide brief summary (1 sentence, key point only)"
            
            prompt += f"Chunk {idx} ({instruction}):\n{remaining_text[:1500]}\n\n"
        
        prompt += "\nIMPORTANT: Return summaries in format: Chunk X: [summary]\nEach summary should be a natural continuation of the previous text, without redundant introductions."
        
        try:
            # ä½¿ç”¨åŒä¸€ä¸ªclientå’Œdeploymentï¼ˆä¸éœ€è¦å•ç‹¬çš„å°æ¨¡å‹ï¼‰
            response = self.client.chat.completions.create(
                model=self.config.azure_deployment_name,
                messages=[{
                    "role": "user",
                    "content": prompt
                }],
                temperature=0.3,
                max_tokens=1000
            )
            
            # è§£æsummaries
            summaries_text = response.choices[0].message.content
            summaries = self._parse_batch_summaries(summaries_text)
            
            return summaries
            
        except Exception as e:
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›ç©ºï¼ˆgraceful degradationï¼‰
            print(f"Warning: Batch summarization failed: {e}")
            return {}
    
    def _parse_batch_summaries(self, summaries_text: str) -> dict:
        """
        è§£ææ‰¹é‡æ€»ç»“çš„è¾“å‡º
        
        Expected format:
        Chunk 0: This section explains...
        Chunk 5: The remaining text discusses...
        """
        import re
        summaries = {}
        
        # æ­£åˆ™æå– "Chunk X: summary"
        pattern = r'Chunk\s+(\d+):\s*(.+?)(?=Chunk\s+\d+:|$)'
        matches = re.findall(pattern, summaries_text, re.DOTALL)
        
        for chunk_idx, summary in matches:
            summaries[int(chunk_idx)] = summary.strip()
        
        return summaries
    
    def _cross_encoder_rerank(
        self,
        query: str,
        chunks: List[RetrievalResult]
    ) -> List[RetrievalResult]:
        """
        ä½¿ç”¨Cross-Encoderå¯¹chunksé‡æ–°æ’åº
        
        Cross-encoderç›¸æ¯”bi-encoderï¼ˆå½“å‰semantic searchï¼‰çš„ä¼˜åŠ¿ï¼š
        - Bi-encoder: queryå’Œdocåˆ†åˆ«ç¼–ç ï¼Œç„¶åè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆå¿«ä½†ä¸å‡†ç¡®ï¼‰
        - Cross-encoder: queryå’Œdocä¸€èµ·ç¼–ç ï¼Œè€ƒè™‘äº¤äº’ï¼ˆæ…¢ä½†å‡†ç¡®ï¼‰
        
        é€‚ç”¨åœºæ™¯ï¼š
        - Retrievalå·²ç»ç¼©å°èŒƒå›´ï¼ˆ15-20 candidatesï¼‰
        - éœ€è¦ç²¾ç¡®æ’åºtop 10ç»™Evidence Judge
        
        Args:
            query: Original question
            chunks: Retrieved chunks from retrieval router
            
        Returns:
            Top-K reranked chunks
        """
        try:
            from sentence_transformers import CrossEncoder
            
            # Lazy load model
            if not hasattr(self, '_cross_encoder'):
                print(f"   Loading cross-encoder model: {self.config.cross_encoder_model}")
                self._cross_encoder = CrossEncoder(self.config.cross_encoder_model)
            
            # Prepare query-document pairs
            pairs = [(query, chunk.text[:512]) for chunk in chunks]  # Limit to 512 chars
            
            # Score all pairs
            ce_scores = self._cross_encoder.predict(pairs)
            
            # Combine with original chunks
            chunk_score_pairs = list(zip(chunks, ce_scores))
            
            # Sort by cross-encoder score (descending)
            chunk_score_pairs.sort(key=lambda x: x[1], reverse=True)
            
            # Keep top-K
            top_k = self.config.cross_encoder_top_k
            reranked_chunks = []
            
            for chunk, ce_score in chunk_score_pairs[:top_k]:
                # Update chunk score to cross-encoder score
                reranked_chunk = RetrievalResult(
                    chunk_id=chunk.chunk_id,
                    score=float(ce_score),  # Replace with CE score
                    text=chunk.text,
                    metadata={
                        **chunk.metadata,
                        "original_score": chunk.score,  # Keep original for debugging
                        "ce_score": float(ce_score)
                    }
                )
                reranked_chunks.append(reranked_chunk)
            
            print(f"   Score range: {ce_scores.max():.4f} (max) â†’ {ce_scores.min():.4f} (min)")
            print(f"   Top 5 CE scores: {[f'{s:.4f}' for s in sorted(ce_scores, reverse=True)[:5]]}")
            
            return reranked_chunks
            
        except ImportError:
            print("   âš ï¸  sentence-transformers not installed, skipping cross-encoder reranking")
            print("   Install with: pip install sentence-transformers")
            return chunks[:self.config.cross_encoder_top_k]
        except Exception as e:
            print(f"   âš ï¸  Cross-encoder reranking failed: {e}")
            return chunks[:self.config.cross_encoder_top_k]
    
    def _save_reranked_chunks(
        self,
        question: str,
        original_chunks: List[RetrievalResult],
        reranked_chunks: List[RetrievalResult],
        metadata: dict
    ) -> None:
        """
        ä¿å­˜Layer 3 rerankingç»“æœï¼Œç”¨äºåˆ†æå’Œè°ƒè¯•
        
        ä¿å­˜å†…å®¹ï¼š
        - Before reranking: åŸå§‹15-20ä¸ªchunksåŠå…¶åˆ†æ•°
        - After reranking: Top 10 chunksåŠå…¶CEåˆ†æ•°
        - Score comparison: æ’åºå˜åŒ–
        
        Args:
            question: Original user question
            original_chunks: Chunks before Layer 3 reranking
            reranked_chunks: Chunks after Layer 3 reranking (top 10)
            metadata: Retrieval metadata
        """
        import json
        from datetime import datetime
        from pathlib import Path
        
        try:
            # Create output directory
            output_dir = Path(self.config.retrieval_output_dir) / "layer3_reranking"
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # Generate filename with timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"layer3_rerank_{timestamp}.json"
            filepath = output_dir / filename
            
            # Prepare comparison data
            comparison = {
                "question": question,
                "timestamp": timestamp,
                "metadata": {
                    "cross_encoder_model": metadata.get("cross_encoder_model", "unknown"),
                    "chunks_before": len(original_chunks),
                    "chunks_after": len(reranked_chunks),
                    "retrieval_mode": metadata.get("mode", "unknown")
                },
                "before_reranking": [
                    {
                        "rank": i + 1,
                        "chunk_id": chunk.chunk_id,
                        "score": chunk.score,
                        "text_preview": chunk.text[:200] + "..." if len(chunk.text) > 200 else chunk.text,
                        "cpt_code": chunk.metadata.get("cpt_code")
                    }
                    for i, chunk in enumerate(original_chunks)
                ],
                "after_reranking": [
                    {
                        "rank": i + 1,
                        "chunk_id": chunk.chunk_id,
                        "ce_score": chunk.score,  # Cross-encoder score
                        "original_score": chunk.metadata.get("original_score", 0.0),
                        "text_preview": chunk.text[:200] + "..." if len(chunk.text) > 200 else chunk.text,
                        "cpt_code": chunk.metadata.get("cpt_code"),
                        "rank_change": self._calculate_rank_change(chunk.chunk_id, original_chunks, i)
                    }
                    for i, chunk in enumerate(reranked_chunks)
                ],
                "score_statistics": {
                    "original_score_range": f"{original_chunks[0].score:.4f} â†’ {original_chunks[-1].score:.4f}",
                    "ce_score_range": f"{reranked_chunks[0].score:.4f} â†’ {reranked_chunks[-1].score:.4f}",
                    "dropped_chunks": len(original_chunks) - len(reranked_chunks)
                }
            }
            
            # Save to file
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(comparison, f, indent=2, ensure_ascii=False)
            
            print(f"   ğŸ’¾ Layer 3 reranking results saved to: {filepath}")
            
        except Exception as e:
            print(f"   âš ï¸  Failed to save reranking results: {e}")
    
    def _calculate_rank_change(
        self,
        chunk_id: str,
        original_chunks: List[RetrievalResult],
        new_rank: int
    ) -> str:
        """
        è®¡ç®—æ’åºå˜åŒ–
        
        Returns:
            "+5" (ä¸Šå‡5ä½), "-3" (ä¸‹é™3ä½), "new" (æ–°è¿›top 10)
        """
        # Find original rank
        for i, chunk in enumerate(original_chunks):
            if chunk.chunk_id == chunk_id:
                old_rank = i
                change = old_rank - new_rank
                if change > 0:
                    return f"+{change}"  # ä¸Šå‡
                elif change < 0:
                    return f"{change}"  # ä¸‹é™
                else:
                    return "0"  # ä¸å˜
        
        return "new"  # ä¸åœ¨åŸå§‹åˆ—è¡¨ä¸­ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
    
    def _extract_chunk_data(self, chunk) -> tuple:
        """æå–chunkæ•°æ®ï¼ˆå…¼å®¹dictå’Œå¯¹è±¡ï¼‰"""
        if isinstance(chunk, dict):
            return (
                chunk.get("chunk_id", "unknown"),
                chunk.get("score", 0.0),
                chunk.get("text", ""),
                chunk.get("metadata", {})
            )
        else:
            return (chunk.chunk_id, chunk.score, chunk.text, chunk.metadata)
   
    