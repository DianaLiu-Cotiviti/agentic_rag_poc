"""
Evidence Judge Agent - è¯æ®è¯„åˆ¤å®˜

è´Ÿè´£è¯„ä¼°æ£€ç´¢åˆ°çš„è¯æ®è´¨é‡ï¼Œåˆ¤æ–­ï¼š
1. è¯æ®æ˜¯å¦å……åˆ†å›ç­”é—®é¢˜ (is_sufficient)
2. è¯æ®è¦†ç›–åº¦ (coverage_score)
3. è¯æ®ç›¸å…³æ€§å’Œå‡†ç¡®æ€§ (specificity_score)
4. æ˜¯å¦å­˜åœ¨çŸ›ç›¾ä¿¡æ¯ (has_contradiction)
5. ç¼ºå¤±çš„æ–¹é¢ (missing_aspects)
"""

from typing import List
from pydantic import BaseModel, Field
from .base import BaseAgent
from ..state import AgenticRAGState, RetrievalResult
from ..prompts.evidence_judge_prompts import (
    EVIDENCE_JUDGE_SYSTEM_MESSAGE,
    build_evidence_judgment_prompt
)
from ..utils.save_workflow_outputs import save_top10_chunks


class EvidenceJudgment(BaseModel):
    """
    Evidence Judgeçš„è¯„ä¼°ç»“æœ
    
    åˆ¤æ–­æ ‡å‡†ï¼š
    - is_sufficient: è¯æ®æ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜ï¼ˆç»¼åˆè€ƒè™‘æ•°é‡ã€è´¨é‡ã€è¦†ç›–åº¦ï¼‰
    - coverage_score: è¯æ®å¯¹é—®é¢˜å„æ–¹é¢çš„è¦†ç›–ç¨‹åº¦ï¼ˆ0.0-1.0ï¼‰
    - specificity_score: è¯æ®çš„ç‰¹å®šæ€§å’Œå‡†ç¡®æ€§ï¼ˆ0.0-1.0ï¼‰
    - has_contradiction: æ£€ç´¢ç»“æœä¸­æ˜¯å¦å­˜åœ¨çŸ›ç›¾ä¿¡æ¯
    - missing_aspects: é—®é¢˜ä¸­æœªè¢«è¦†ç›–çš„æ–¹é¢ï¼ˆç”¨äºæŒ‡å¯¼é‡è¯•ï¼‰
    - reasoning: è¯„ä¼°æ¨ç†è¿‡ç¨‹ï¼ˆè§£é‡Šä¸ºä»€ä¹ˆsufficient/insufficientï¼‰
    """
    is_sufficient: bool = Field(
        description="Whether the evidence is sufficient to answer the question"
    )
    coverage_score: float = Field(
        ge=0.0,
        le=1.0,
        description="How well the evidence covers different aspects of the question (0.0-1.0)"
    )
    specificity_score: float = Field(
        ge=0.0,
        le=1.0,
        description="How specific and accurate the evidence is (0.0-1.0)"
    )
    has_contradiction: bool = Field(
        description="Whether there are contradictory statements in the evidence"
    )
    missing_aspects: List[str] = Field(
        default_factory=list,
        description="Aspects of the question not covered by current evidence"
    )
    reasoning: str = Field(
        description="Detailed reasoning for the sufficiency judgment"
    )


class EvidenceJudgeAgent(BaseAgent):
    """
    Evidence Judge Agent - è¯„ä¼°æ£€ç´¢è¯æ®è´¨é‡
    
    æ ¸å¿ƒèŒè´£ï¼š
    1. åˆ¤æ–­è¯æ®æ˜¯å¦å……åˆ† (is_sufficient)
       - è€ƒè™‘é—®é¢˜ç±»å‹ï¼ˆç®€å•CPT lookup vs å¤æ‚billingè§„åˆ™ï¼‰
       - è€ƒè™‘è¯æ®æ•°é‡å’Œè´¨é‡
       - è€ƒè™‘è¦†ç›–åº¦
    
    2. è¯„ä¼°è¯æ®è´¨é‡æŒ‡æ ‡ï¼š
       - coverage_score: è¦†ç›–é—®é¢˜çš„å¤šä¸ªæ–¹é¢ï¼ˆCPT codeå®šä¹‰ã€modifierã€bundlingç­‰ï¼‰
       - specificity_score: è¯æ®çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§
    
    3. è¯†åˆ«é—®é¢˜ï¼š
       - has_contradiction: æ£€æµ‹çŸ›ç›¾ä¿¡æ¯
       - missing_aspects: è¯†åˆ«ç¼ºå¤±çš„æ–¹é¢
    
    4. æŒ‡å¯¼ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š
       - å¦‚æœinsufficientï¼Œmissing_aspectsæŒ‡å¯¼query refinement
       - å¦‚æœsufficientï¼Œé«˜è´¨é‡chunksç”¨äºanswer generation
    """
    
    def __init__(self, config, client=None):
        """
        Args:
            config: Configuration object with Azure OpenAI settings
            client: Azure OpenAI client (optional, will use config.client if not provided)
        """
        self.config = config
        self._client = client if client is not None else getattr(config, 'client', None)
        
        # Initialize retrieval tools for cross-encoder reranking
        from ..tools.retrieval_tools import RetrievalTools
        self.retrieval_tools = RetrievalTools(config)
    
    @property
    def client(self):
        """Lazy initialization of LLM client"""
        if self._client is None:
            from openai import AzureOpenAI
            self._client = AzureOpenAI(
                api_key=self.config.azure_openai_api_key,
                api_version=self.config.azure_api_version,
                azure_endpoint=self.config.azure_openai_endpoint
            )
        return self._client
    
    def process(self, state: AgenticRAGState) -> dict:
        """
        è¯„ä¼°æ£€ç´¢è¯æ®è´¨é‡
        
        Args:
            state: Contains question, question_type, retrieved_chunks, cpt_descriptions
            
        Returns:
            dict: Contains evidence_assessment
        """
        question = state["question"]
        question_type = state.get("question_type", "general")
        chunks = state.get("retrieved_chunks", [])
        retrieval_metadata = state.get("retrieval_metadata", {})
        cpt_descriptions = state.get("cpt_descriptions", {})  # Get CPT descriptions from state
        
        # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°å†…å®¹ - æ˜ç¡®insufficient
        if not chunks:
            return {
                "evidence_assessment": {
                    "is_sufficient": False,
                    "coverage_score": 0.0,
                    "specificity_score": 0.0,
                    "has_contradiction": False,
                    "missing_aspects": ["No chunks retrieved - all aspects missing"],
                    "reasoning": "No relevant chunks were retrieved. Need to refine query or adjust retrieval strategy."
                }
            }
        
        # Apply cross-encoder reranking if enabled
        reranked_chunks = chunks  # Keep original for comparison
        if self.config.use_cross_encoder_rerank and len(chunks) > self.config.cross_encoder_top_k:
            print(f"\nğŸ”„ Layer 3 Reranking: Cross-Encoder (Question-aware)")
            print(f"   Purpose: Refine {len(chunks)} chunks to top {self.config.cross_encoder_top_k} based on original question")
            print(f"   Before: {len(chunks)} chunks (from Layer 1-2 fusion)")
            
            # Call cross-encoder reranking tool
            reranked_chunks = self.retrieval_tools.cross_encoder_rerank(
                query=question,
                chunks=chunks,
                top_k=self.config.cross_encoder_top_k
            )
            
            print(f"   After: {len(reranked_chunks)} chunks (optimized for Evidence Judge)")
            
            # Update metadata
            retrieval_metadata["cross_encoder_reranked"] = True
            retrieval_metadata["cross_encoder_model"] = self.config.cross_encoder_model
            retrieval_metadata["chunks_before_layer3"] = len(chunks)
            retrieval_metadata["chunks_after_layer3"] = len(reranked_chunks)
            
            # ä¿å­˜top 10 chunksä½œä¸ºLLMå›ç­”çš„ä¾æ®
            mode = retrieval_metadata.get('mode', 'unknown')
            save_path = save_top10_chunks(
                top10_chunks=reranked_chunks,
                question=state.get('question', ''),
                output_dir=self.config.retrieval_output_dir,
                metadata={
                    'mode': mode,
                    'original_chunks_count': len(chunks),
                    'reranked_to_top': len(reranked_chunks),
                    'layer': 'layer3_cross_encoder'
                }
            )
            print(f"   ğŸ’¾ Top 10 chunks saved to: {save_path}")
        else:
            # Cross-encoder disabled or not enough chunks - use score-based top-K
            if not self.config.use_cross_encoder_rerank:
                print(f"\nâ­ï¸  Layer 3 Reranking: Skipped (disabled in config)")
            else:
                print(f"\nâ­ï¸  Layer 3 Reranking: Skipped (only {len(chunks)} chunks, threshold is {self.config.cross_encoder_top_k})")
            
            # Still limit to top-K based on existing scores (from Layer 1-2)
            if len(chunks) > self.config.cross_encoder_top_k:
                reranked_chunks = chunks[:self.config.cross_encoder_top_k]
                print(f"   ğŸ“Š Using top {self.config.cross_encoder_top_k} chunks based on Layer 1-2 scores")
                retrieval_metadata["cross_encoder_reranked"] = False
                retrieval_metadata["truncated_to_top_k"] = True
            else:
                print(f"   ğŸ“Š Using all {len(chunks)} chunks (no truncation needed)")
                retrieval_metadata["cross_encoder_reranked"] = False
            
            # ä¿å­˜top 10 chunksä½œä¸ºLLMå›ç­”çš„ä¾æ® (score-basedç‰ˆæœ¬)
            mode = retrieval_metadata.get('mode', 'unknown')
            save_path = save_top10_chunks(
                top10_chunks=reranked_chunks,
                question=state.get('question', ''),
                output_dir=self.config.retrieval_output_dir,
                metadata={
                    'mode': mode,
                    'original_chunks_count': len(chunks),
                    'reranked_to_top': len(reranked_chunks),
                    'layer': 'layer1_layer2_score_based'
                }
            )
            print(f"   ğŸ’¾ Top {len(reranked_chunks)} chunks saved to: {save_path}")

        
        # Use reranked chunks for evaluation
        chunks_to_judge = reranked_chunks
        
        # æ„å»ºpromptç”¨äºLLMè¯„ä¼°
        # æ³¨æ„ï¼šåªç”¨ original question å’Œ retrieved chunks è¯„ä¼°
        # ä¸éœ€è¦ sub-queriesï¼ˆå®ƒä»¬åªæ˜¯æ£€ç´¢æ‰‹æ®µï¼Œä¸æ˜¯è¯„ä¼°ç›®æ ‡ï¼‰
        prompt = self._build_judgment_prompt(
            question=question,
            question_type=question_type,
            chunks=chunks_to_judge,
            retrieval_metadata=retrieval_metadata,
            cpt_descriptions=cpt_descriptions  # Pass CPT descriptions to prompt builder
        )
        
        # è°ƒç”¨LLMè¿›è¡Œç»“æ„åŒ–è¯„ä¼°
        response = self.client.beta.chat.completions.parse(
            model=self.config.azure_deployment_name,
            messages=[
                {"role": "system", "content": EVIDENCE_JUDGE_SYSTEM_MESSAGE},
                {"role": "user", "content": prompt}
            ],
            response_format=EvidenceJudgment,
            temperature=self.config.agent_temperature
        )
        
        judgment = response.choices[0].message.parsed
        
        # Return updated state with reranked chunks
        return {
            "evidence_assessment": {
                "is_sufficient": judgment.is_sufficient,
                "coverage_score": judgment.coverage_score,
                "specificity_score": judgment.specificity_score,
                "has_contradiction": judgment.has_contradiction,
                "missing_aspects": judgment.missing_aspects,
                "reasoning": judgment.reasoning
            },
            "retrieved_chunks": reranked_chunks,  # Update state with top-10 reranked chunks
            "retrieval_metadata": retrieval_metadata  # Update metadata with Layer 3 info
        }
    
    def _build_judgment_prompt(
        self,
        question: str,
        question_type: str,
        chunks: List[RetrievalResult],
        retrieval_metadata: dict,
        cpt_descriptions: dict = None
    ) -> str:
        """
        æ„å»ºEvidence Judgeçš„è¯„ä¼°prompt
        
        è¯„ä¼°é€»è¾‘ï¼š
        - è¯„ä¼°ç›®æ ‡ï¼šoriginal questionï¼ˆä¸æ˜¯sub-queriesï¼‰
        - è¯„ä¼°è¯æ®ï¼šretrieved chunks + CPT descriptionsï¼ˆå·²èåˆï¼‰
        - è¯„ä¼°æ ‡å‡†ï¼šquestion_type å¯¹åº”çš„ required aspects
        
        Args:
            question: Original user questionï¼ˆè¯„ä¼°ç›®æ ‡ï¼‰
            question_type: Question type
            chunks: Retrieved chunksï¼ˆå·²èåˆçš„15-20ä¸ªchunksï¼‰
            retrieval_metadata: Retrieval metadata
            cpt_descriptions: CPT code -> description mapping (from retrieval)
        """
        # Format chunks
        chunks_text = self._format_chunks_for_evaluation(chunks)
        
        # Format CPT descriptions (if available)
        cpt_desc_text = ""
        if cpt_descriptions:
            cpt_desc_text = "\n\n### ğŸ“‹ CPT Code Definitions (Retrieved)\n\n"
            for code, description in cpt_descriptions.items():
                cpt_desc_text += f"**CPT {code}**: {description}\n\n"
        
        # Extract metadata
        retrieval_mode = retrieval_metadata.get("mode", "unknown")
        strategies_used = retrieval_metadata.get("strategies_used", "N/A")
        
        # Use centralized prompt builder
        return build_evidence_judgment_prompt(
            question=question,
            question_type=question_type,
            chunks_text=chunks_text + cpt_desc_text,  # Append CPT descriptions to chunks
            retrieval_mode=retrieval_mode,
            strategies_used=str(strategies_used),
            total_chunks=len(chunks)
        )
    
    def _format_chunks_for_evaluation(self, chunks: List[RetrievalResult]) -> str:
        """
        åˆ†å±‚å±•ç¤º top 10 chunks + LLMæ€»ç»“æˆªæ–­éƒ¨åˆ†ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
        
        ç­–ç•¥ï¼ˆåˆ†å±‚å±•ç¤º + LLMæ€»ç»“ï¼‰ï¼š
        - Chunk 1-5 (Detailed): å‰800å­—ç¬¦ + LLM 2-3å¥æ€»ç»“å‰©ä½™éƒ¨åˆ†ï¼ˆå¦‚æœè¢«æˆªæ–­ï¼‰
        - Chunk 6-10 (Medium): å‰400å­—ç¬¦ + LLM 1-2å¥æ€»ç»“å‰©ä½™éƒ¨åˆ†ï¼ˆå¦‚æœè¢«æˆªæ–­ï¼‰
        - æ‰¹é‡è°ƒç”¨ï¼š1æ¬¡LLMè°ƒç”¨å¤„ç†æ‰€æœ‰æˆªæ–­éƒ¨åˆ†çš„æ€»ç»“
        
        Args:
            chunks: List of RetrievalResult objects (top 10 after Layer 3 reranking)
        """
        if not chunks:
            return "No chunks retrieved."
        
        # Step 1: æ”¶é›†æ‰€æœ‰éœ€è¦æ€»ç»“çš„æˆªæ–­éƒ¨åˆ†
        chunks_to_summarize = []
        for i, chunk in enumerate(chunks[:10]):  # Only top 10 after Layer 3 reranking
            chunk_id, score, text, metadata = self._extract_chunk_data(chunk)
            
            # æ ¹æ®tierå†³å®šå±•ç¤ºé•¿åº¦ï¼ˆåªæœ‰ top 10ï¼‰
            if i < 5:
                preview_len = 800
                tier = "detailed"  # Chunk 1-5: 800 chars + 2-3 sentence summary
            else:
                preview_len = 400
                tier = "medium"  # Chunk 6-10: 400 chars + 1-2 sentence summary
            
            # å¦‚æœæ–‡æœ¬è¢«æˆªæ–­ï¼Œè®°å½•éœ€è¦æ€»ç»“çš„éƒ¨åˆ†
            if len(text) > preview_len:
                chunks_to_summarize.append({
                    "index": i,
                    "tier": tier,
                    "preview": text[:preview_len],
                    "remaining": text[preview_len:],  # è¢«æˆªæ–­çš„éƒ¨åˆ†
                    "chunk_id": chunk_id,
                    "score": score,
                    "metadata": metadata
                })
            else:
                chunks_to_summarize.append({
                    "index": i,
                    "tier": tier,
                    "preview": text,
                    "remaining": None,  # æ— éœ€æ€»ç»“
                    "chunk_id": chunk_id,
                    "score": score,
                    "metadata": metadata
                })
        
        # Step 2: æ‰¹é‡æ€»ç»“æ‰€æœ‰æˆªæ–­éƒ¨åˆ†ï¼ˆ1æ¬¡LLMè°ƒç”¨ï¼‰
        summaries = self._batch_summarize_truncated_parts(chunks_to_summarize)
        
        # Step 3: æ ¼å¼åŒ–å±•ç¤ºï¼ˆå°†æ€»ç»“ç›´æ¥appendåˆ°previewåé¢ï¼‰
        formatted_chunks = []
        
        for item in chunks_to_summarize:
            i = item["index"]
            tier = item["tier"]
            chunk_id = item["chunk_id"]
            score = item["score"]
            metadata = item["metadata"]
            preview = item["preview"]
            summary = summaries.get(i, "")  # è·å–è¯¥chunkçš„æ€»ç»“
            
            # å°†æ€»ç»“ç›´æ¥æ‹¼æ¥åˆ°previewåé¢ï¼Œå½¢æˆå®Œæ•´æ–‡æœ¬
            full_text = f"{preview} {summary}" if summary else preview
            
            cpt_info = f" [CPT: {metadata.get('cpt_code')}]" if metadata.get("cpt_code") else ""
            
            # æ ¹æ®tieræ ¼å¼åŒ–ï¼ˆæ€»ç»“å·²appendåˆ°full_textä¸­ï¼‰
            if tier == "detailed":
                formatted_chunks.append(
                    f"**[Detailed {i+1}]** (ID: {chunk_id}, Score: {score:.4f}){cpt_info}\n"
                    f"{full_text}"
                )
            else:
                formatted_chunks.append(
                    f"**[Medium {i+1}]** (Score: {score:.4f}){cpt_info}\n"
                    f"{full_text}"
                )
        
        # Step 4: æ•´ä½“ç»Ÿè®¡ä¿¡æ¯
        summary_info = f"\n\n**ğŸ“Š Overall Summary**\n"
        summary_info += f"Total chunks analyzed: {len(chunks)}\n"
        summary_info += f"Score range: {chunks[0].score:.4f} (highest) to {chunks[len(chunks)-1].score:.4f} (lowest)"
        
        return "\n\n".join(formatted_chunks) + summary_info
    
    def _batch_summarize_truncated_parts(self, chunks_data: List[dict]) -> dict:
        """
        æ‰¹é‡æ€»ç»“æ‰€æœ‰è¢«æˆªæ–­çš„chunkéƒ¨åˆ†ï¼ˆ1æ¬¡LLMè°ƒç”¨ï¼‰
        
        ä½¿ç”¨é…ç½®çš„ä¸»æ¨¡å‹è¿›è¡Œæ€»ç»“
        
        Args:
            chunks_data: List of chunk data with 'remaining' text to summarize
            
        Returns:
            dict: {chunk_index: summary_text}
        """
        # æ”¶é›†éœ€è¦æ€»ç»“çš„chunks
        to_summarize = [
            (item["index"], item["tier"], item["remaining"])
            for item in chunks_data
            if item["remaining"]  # åªæ€»ç»“è¢«æˆªæ–­çš„éƒ¨åˆ†
        ]
        
        if not to_summarize:
            return {}  # æ— éœ€æ€»ç»“
        
        # æ„å»ºæ‰¹é‡æ€»ç»“prompt
        prompt = "Summarize the continuation of each chunk below. Be concise but capture key medical coding details.\n\n"
        
        for idx, tier, remaining_text in to_summarize:
            # æ ¹æ®tierå†³å®šæ€»ç»“é•¿åº¦å’Œè¦æ±‚
            if tier == "detailed":
                instruction = "Provide detailed summary (2-3 sentences covering key medical coding details)"
            else:
                instruction = "Provide medium summary (1-2 sentences with main points)"
            
            prompt += f"Chunk {idx} ({instruction}):\n{remaining_text[:1500]}\n\n"
        
        prompt += "\nIMPORTANT: Return summaries in format: Chunk X: [summary]\nEach summary should be a natural continuation of the previous text, without redundant introductions."
        
        try:
            # ä½¿ç”¨åŒä¸€ä¸ªclientå’Œdeploymentï¼ˆä¸éœ€è¦å•ç‹¬çš„å°æ¨¡å‹ï¼‰
            response = self.client.chat.completions.create(
                model=self.config.azure_deployment_name,
                messages=[{
                    "role": "user",
                    "content": prompt
                }],
                temperature=0.3,
                max_tokens=1000
            )
            
            # è§£æsummaries
            summaries_text = response.choices[0].message.content
            summaries = self._parse_batch_summaries(summaries_text)
            
            return summaries
            
        except Exception as e:
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›ç©ºï¼ˆgraceful degradationï¼‰
            print(f"Warning: Batch summarization failed: {e}")
            return {}
    
    def _parse_batch_summaries(self, summaries_text: str) -> dict:
        """
        è§£ææ‰¹é‡æ€»ç»“çš„è¾“å‡º
        
        Expected format:
        Chunk 0: This section explains...
        Chunk 5: The remaining text discusses...
        """
        import re
        summaries = {}
        
        # æ­£åˆ™æå– "Chunk X: summary"
        pattern = r'Chunk\s+(\d+):\s*(.+?)(?=Chunk\s+\d+:|$)'
        matches = re.findall(pattern, summaries_text, re.DOTALL)
        
        for chunk_idx, summary in matches:
            summaries[int(chunk_idx)] = summary.strip()
        
        return summaries
    
    def _extract_chunk_data(self, chunk) -> tuple:
        """æå–chunkæ•°æ®ï¼ˆå…¼å®¹dictå’Œå¯¹è±¡ï¼‰"""
        if isinstance(chunk, dict):
            return (
                chunk.get("chunk_id", "unknown"),
                chunk.get("score", 0.0),
                chunk.get("text", ""),
                chunk.get("metadata", {})
            )
        else:
            return (chunk.chunk_id, chunk.score, chunk.text, chunk.metadata)

   
    