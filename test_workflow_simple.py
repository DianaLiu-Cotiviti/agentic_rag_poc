"""
æµ‹è¯•Simple Agentic RAG Workflow
================================

éªŒè¯æ•´ä¸ªagent pipelineä»å¤´åˆ°å°¾æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š
User Query â†’ Orchestrator â†’ Query Planner â†’ Retrieval Router â†’ Evidence Judge

æ³¨æ„ï¼šWorkflowè‡ªåŠ¨ä¿å­˜æ‰§è¡Œç»“æœåˆ°memory/ç›®å½•
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.config import AgenticRAGConfig
from src.workflow_simple import SimpleAgenticRAGWorkflow


def test_simple_workflow():
    """æµ‹è¯•ç®€åŒ–workflowçš„å®Œæ•´æµç¨‹"""
    
    print("="*80)
    print("ğŸ§ª Testing Simple Agentic RAG Workflow")
    print("="*80)
    
    # ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
    config = AgenticRAGConfig.from_env()
    
    print(f"\nğŸ“‹ Configuration:")
    print(f"   Retrieval Mode: {config.retrieval_mode}")
    print(f"   Top K: {config.top_k}")
    print(f"   Memory Dir: {config.memory_dir}")
    
    # åˆ›å»ºworkflowï¼ˆè‡ªåŠ¨å¯ç”¨memoryï¼‰
    workflow = SimpleAgenticRAGWorkflow(config, enable_memory=True)
    
    # æµ‹è¯•é—®é¢˜
    test_question = "What scenarios can't be reported with CPT code 44180?"
    test_cpt_code = 44180
    
    print(f"\nâ“ Test Question: {test_question}")
    print(f"ğŸ¥ CPT Code: {test_cpt_code}")
    
    try:
        # è¿è¡Œworkflowï¼ˆè‡ªåŠ¨ä¿å­˜åˆ°memoryï¼‰
        result = workflow.run(question=test_question)
        
        print("\n" + "="*80)
        print("ğŸ“Š Final State Summary")
        print("="*80)
        
        # Show retry information
        retry_count = result.get('retry_count', 0)
        if retry_count > 0:
            print(f"\nğŸ”„ Retry Information:")
            print(f"   Total Retries: {retry_count}")
            print(f"   Total Rounds: {retry_count + 1} (initial + {retry_count} retry)")
        
        print(f"\n1ï¸âƒ£  Orchestrator:")
        print(f"   Question Type: {result.get('question_type')}")
        print(f"   Complexity: {result.get('question_complexity')}")
        print(f"   Strategy Hints: {result.get('retrieval_strategies')}")
        
        print(f"\n2ï¸âƒ£  Query Planner:")
        query_candidates = result.get('query_candidates', [])
        print(f"   Query Candidates: {len(query_candidates)}")
        for i, qc in enumerate(query_candidates, 1):
            # qc is a QueryCandidate object
            query_text = qc.query if hasattr(qc, 'query') else str(qc)
            print(f"      {i}. {query_text}")
        
        print(f"\n3ï¸âƒ£  Retrieval Router:")
        chunks = result.get('retrieved_chunks', [])
        print(f"   Retrieved Chunks: {len(chunks)}")
        metadata = result.get('retrieval_metadata', {})
        print(f"   Strategies Used: {metadata.get('strategies_used', 'N/A')}")
        
        print(f"\n4ï¸âƒ£  Evidence Judge:")
        assessment = result.get('evidence_assessment', {})
        print(f"   Is Sufficient: {assessment.get('is_sufficient')}")
        print(f"   Coverage: {assessment.get('coverage_score', 0):.2f}")
        print(f"   Specificity: {assessment.get('specificity_score', 0):.2f}")
        
        if not assessment.get('is_sufficient'):
            missing = assessment.get('missing_aspects', [])
            if missing:
                print(f"   Missing Aspects ({len(missing)}):")
                for aspect in missing:
                    print(f"      â€¢ {aspect}")
        
        # Show Answer Generator output if available
        final_answer = result.get('final_answer')
        if final_answer:
            print(f"\n5ï¸âƒ£  Answer Generator:")
            print(f"   Answer Preview: {final_answer.get('answer', 'N/A')[:150]}...")
            print(f"   Key Points: {len(final_answer.get('key_points', []))}")
            citation_map = final_answer.get('citation_map', {})
            print(f"   Citations: {len(citation_map)} chunks")
            print(f"   Confidence: {final_answer.get('confidence', 0):.2f}")
            if final_answer.get('limitations'):
                print(f"   Limitations: {len(final_answer.get('limitations', []))} noted")
        else:
            print(f"\n5ï¸âƒ£  Answer Generator:")
            print(f"   Skipped (evidence insufficient)")
        
        print("\n" + "="*80)
        print("âœ… All steps completed successfully!")
        print("="*80)
        
        # éªŒè¯å…³é”®å­—æ®µ
        checks = [
            ("Orchestrator set question_type", result.get('question_type') is not None),
            ("Orchestrator set complexity", result.get('question_complexity') is not None),
            ("Orchestrator provided strategy hints", len(result.get('retrieval_strategies', [])) > 0),
            ("Query Planner generated queries", len(query_candidates) > 0),
            ("Retrieval Router returned chunks", len(chunks) > 0),
            ("Evidence Judge provided assessment", assessment.get('is_sufficient') is not None),
            ("Coverage score in valid range", 0 <= assessment.get('coverage_score', -1) <= 1),
            ("Specificity score in valid range", 0 <= assessment.get('specificity_score', -1) <= 1),
        ]
        
        # Add answer generator check if evidence was sufficient
        if assessment.get('is_sufficient'):
            checks.append(("Answer Generator provided answer", final_answer is not None))
            # Check citation_map (dict) instead of citations (list)
            checks.append(("Answer has citations", len(final_answer.get('citation_map', {})) > 0 if final_answer else False))
        
        print("\nğŸ“‹ Validation Checks:")
        all_passed = True
        for i, (desc, passed) in enumerate(checks, 1):
            status = "âœ…" if passed else "âŒ"
            print(f"   [{i}] {status} {desc}")
            all_passed = all_passed and passed
        
        if all_passed:
            print("\nğŸ‰ All validation checks passed!")
            return True
        else:
            print("\nâš ï¸  Some validation checks failed!")
            return False
            
    except Exception as e:
        print(f"\nâŒ Error during workflow execution: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_multiple_modes():
    """æµ‹è¯•ä¸åŒçš„retrievalæ¨¡å¼"""
    
    print("\n" + "="*80)
    print("ğŸ”„ Testing Different Retrieval Modes")
    print("="*80)
    
    modes = ["direct", "planning"]  # tool_callingéœ€è¦æ›´å¤šLLMè°ƒç”¨
    test_question = "What is CPT code 14301?"
    
    results = {}
    
    for mode in modes:
        print(f"\n{'='*80}")
        print(f"Testing mode: {mode}")
        print('='*80)
        
        try:
            config = AgenticRAGConfig.from_env()
            config.retrieval_mode = mode
            
            workflow = SimpleAgenticRAGWorkflow(config, enable_memory=True)
            result = workflow.run(question=test_question)
            
            metadata = result.get('retrieval_metadata', {})
            assessment = result.get('evidence_assessment', {})
            
            results[mode] = {
                'success': True,
                'chunks': len(result.get('retrieved_chunks', [])),
                'strategies': metadata.get('strategies_used', []),
                'coverage': assessment.get('coverage_score', 0),
                'specificity': assessment.get('specificity_score', 0)
            }
            
            print(f"âœ… {mode} mode completed")
            print(f"   Chunks: {results[mode]['chunks']}")
            print(f"   Coverage: {results[mode]['coverage']:.2f}")
            
        except Exception as e:
            print(f"âŒ {mode} mode failed: {e}")
            results[mode] = {'success': False, 'error': str(e)}
    
    # å¯¹æ¯”ç»“æœ
    print("\n" + "="*80)
    print("ğŸ“Š Mode Comparison")
    print("="*80)
    
    for mode, data in results.items():
        if data['success']:
            print(f"\n{mode.upper()}:")
            print(f"  Chunks: {data['chunks']}")
            print(f"  Strategies: {data['strategies']}")
            print(f"  Coverage: {data['coverage']:.2f}")
            print(f"  Specificity: {data['specificity']:.2f}")
        else:
            print(f"\n{mode.upper()}: âŒ Failed - {data['error']}")
    
    return all(r['success'] for r in results.values())


if __name__ == "__main__":
    # Test 1: åŸºç¡€workflowæµ‹è¯•
    success = test_simple_workflow()
    
    # Test 2: å¤šæ¨¡å¼å¯¹æ¯”æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
    # Uncomment to run mode comparison
    # success = success and test_multiple_modes()
    
    sys.exit(0 if success else 1)
