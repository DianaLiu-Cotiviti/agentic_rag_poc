# Agentic RAG System ğŸ¤–

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![LangGraph](https://img.shields.io/badge/LangGraph-latest-green.svg)](https://github.com/langchain-ai/langgraph)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

A production-ready multi-agent Retrieval-Augmented Generation (RAG) system built with LangGraph, specifically designed for NCCI medical coding policy document Q&A.

## âœ¨ Key Features

- ğŸ§  **Multi-Agent Collaboration**: 5 specialized agents working intelligently together
- ğŸ” **Triple Retrieval Modes**: Direct (fast) / Planning (balanced) / Tool Calling (intelligent)
- ğŸ¯ **Hybrid Retrieval**: Range Routing + BM25 + Semantic Vector + RRF Fusion
- ğŸ“Š **Evidence Quality Assessment**: Automated evidence sufficiency evaluation with retry mechanism
- ğŸ’¾ **Complete Memory System**: Full execution history with timestamp-based storage
- ğŸ”„ **Auto Index Management**: Intelligent index checking and building
- ğŸ“¦ **Structured Output**: Answers + citations + confidence scores + execution logs

## ğŸ—ï¸ System Architecture

### Workflow Overview

```
User Question
    â†“
ğŸ”§ Preprocessing: Auto-check and build indexes (Range + BM25 + ChromaDB)
    â†“
ğŸ§  Orchestrator Agent (Intent Analysis, Strategy Hints)
    â†“
ğŸ“‹ Query Planner Agent (Generate 4 Query Candidates)
    â†“
ğŸ” Retrieval Router (3 Modes: Direct/Planning/Tool Calling)
    â”œâ”€ Direct Mode: Fixed pipeline (0 LLM calls, ~0.5s)
    â”œâ”€ Planning Mode: LLM-generated plan (1 LLM call, ~2s)
    â””â”€ Tool Calling Mode: Agentic iteration (5-15 LLM calls, ~10s)
    â†“
âš–ï¸  Evidence Judge Agent (Quality Assessment)
    â†“
ğŸ’¾ Memory System (Save complete execution history)
    â†“
Final Results + Retrieved Chunks
```

### Three Retrieval Modes

| Mode | LLM Calls | Speed | Cost | Intelligence | Use Case |
|------|-----------|-------|------|--------------|----------|
| **direct** | 0 | ~0.5s | $0 | âš¡ | Production (speed priority) |
| **planning** | 1 | ~2s | $0.01 | ğŸ¤–ğŸ¤– | Standard (balanced) |
| **tool_calling** | 5-15 | ~10s | $0.05+ | ğŸ¤–ğŸ¤–ğŸ¤– | Research (quality priority) |

### Core Components

#### 1. Agents (`src/agents/`)

| Agent | File | Responsibility |
|-------|------|----------------|
| **Orchestrator** | `orchestrator.py` | Analyze question type, complexity, provide strategy hints |
| **Query Planner** | `query_planner.py` | Generate 4 query candidates (original, section-specific, synonym, constraint-focused) |
| **Retrieval Router** | `retrieval_router*.py` | Execute retrieval in 3 modes (direct/planning/tool_calling) |
| **Evidence Judge** | `evidence_judge.py` | Assess coverage, specificity, citations; identify missing aspects |

#### 2. Retrieval Tools (`src/tools/`)

| Tool | File | Function |
|------|------|----------|
| **Range Routing** | `retrieval_tools.py` | CPT code range filtering (SQLite-based) |
| **BM25 Search** | `bm25_store.py` | Lexical keyword search |
| **Semantic Search** | `chroma_store.py` | Vector similarity search (ChromaDB) |
| **Hybrid Search** | `retrieval_tools.py` | BM25 + Semantic RRF fusion |
| **Index Builder** | `build_indexes.py` | Auto-check and build all indexes |

#### 3. Configuration (`src/config.py`)

Centralized configuration with lazy client initialization:

```python
class AgenticRAGConfig:
    # Paths
    chunks_path: str = "rag/build/chunks.jsonl"
    range_index_path: str = "rag/build/cpt_range_index.db"
    bm25_index_path: str = "rag/build/bm25_index.pkl"
    chroma_db_path: str = "rag/build/chroma_db"
    
    # Retrieval mode
    retrieval_mode: str = "tool_calling"  # direct/planning/tool_calling
    
    # Lazy clients (shared across components)
    @property
    def client(self): ...  # Azure OpenAI client
    
    @property
    def embedding_client(self): ...  # Embedding client
    
    @property
    def chroma_client(self): ...  # ChromaDB client
```

#### 4. Memory System (`src/memory.py`)

Complete execution history with structured storage:

```python
WorkflowMemory.save_execution(
    question="What is CPT 14301?",
    final_state=state,
    workflow_type="simple",
    mode="tool_calling"
)
# Saves to: memory/workflow_simple_tool_calling_20260205_171201.json
#           memory/latest_simple_tool_calling.json
```

#### 5. Workflow Engine (`src/workflow_simple.py`)

Linear workflow (no retry) for testing and validation:

- Step 1: Orchestrator â†’ Question analysis
- Step 2: Query Planner â†’ Generate 4 candidates  
- Step 3: Retrieval Router â†’ Execute retrieval
- Step 4: Evidence Judge â†’ Quality assessment
- Auto-save: Memory + Retrieved chunks

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Azure OpenAI API access (with separate embedding endpoint)

### 1. Install Dependencies

```bash
# Clone the repository
git clone <repository-url>
cd agentic_rag

# Activate virtual environment
source agentic_rag/bin/activate  # macOS/Linux
# or agentic_rag\Scripts\activate  # Windows

# Install dependencies (if needed)
pip install -r requirements.txt
```

### 2. Configure Environment Variables

Create a `.env` file with **two separate Azure OpenAI endpoints**:

```bash
# Chat/Completion Endpoint
AZURE_OPENAI_API_KEY=your-chat-api-key
AZURE_OPENAI_ENDPOINT=https://your-chat-endpoint.openai.azure.com/
AZURE_API_VERSION=2024-12-01-preview
AZURE_DEPLOYMENT_NAME=gpt-4o

# Embedding Endpoint (separate)
AZURE_OPENAI_API_KEY_EMBEDDING=your-embedding-api-key
AZURE_OPENAI_ENDPOINT_EMBEDDING=https://your-embedding-endpoint.openai.azure.com/
AZURE_API_VERSION_EMBEDDING=2024-02-15-preview
AZURE_DEPLOYMENT_NAME_EMBEDDING=text-embedding-3-large-2
```

### 3. Build Indexes (Auto-check on first run)

```bash
# Check and build missing indexes
python -m src.tools.build_indexes

# Force rebuild all indexes
python -m src.tools.build_indexes --force
```

Output:
```
ğŸ“¦ Checking and Building Indexes...
âœ“ Range Index already exists: rag/build/cpt_range_index.db
âœ“ BM25 Index already exists: rag/build/bm25_index.pkl
âœ“ ChromaDB Index already exists: rag/build/chroma_db
âœ… All indexes ready!
```

### 4. Run Test Workflow

```bash
python test_workflow_simple.py
```

Sample output:
```
ğŸ§ª Testing Simple Agentic RAG Workflow
================================================================================

ğŸ“‹ Configuration:
   Retrieval Mode: tool_calling
   Top K: 15
   Memory Dir: memory

ğŸ”§ Preprocessing: Ensuring all indexes are built...
âœ… All indexes ready!

ğŸ¯ Step 1: Orchestrator - Analyzing question...
Question Type: PTP
Complexity: medium
Strategy Hints: ['range_routing', 'bm25', 'semantic']

ğŸ“‹ Step 2: Query Planner - Generating query candidates...
Generated 4 query candidates:
  1. What is CPT code 14301 and when can it be billed with 27702?
  2. NCCI procedure-to-procedure edits for CPT 14301 and 27702
  3. Can adjacent tissue transfer CPT 14301 be billed together with tibial osteotomy CPT 27702?
  4. Billing restrictions and modifier indicators for CPT 14301 and 27702

ğŸ” Step 3: Retrieval Router - Executing retrieval...
Mode: tool_calling

  ğŸ”„ Tool Calling Iteration #1
     â†’ range_routing(cpt_code=14301) â†’ 50 chunks
     â†’ range_routing(cpt_code=27702) â†’ 15 chunks

  ğŸ”„ Tool Calling Iteration #2
     â†’ bm25_search(...) â†’ 20 chunks
     â†’ semantic_search(...) â†’ 20 chunks

  ğŸ”„ Tool Calling Iteration #3
     â†’ rrf_fusion(result_ids=['bm25_0', 'semantic_0']) â†’ 20 chunks

  âœ… LLM finished tool calling

ğŸ“Š Tool Calling Execution Summary:
   Total iterations: 4
   Total tool calls: 5
   Final results: 20 chunks retrieved

âš–ï¸  Step 4: Evidence Judge - Assessing evidence quality...
Is Sufficient: False
Coverage Score: 0.30
Specificity Score: 0.30

ğŸ’¾ Workflow result saved to: memory/workflow_simple_tool_calling_20260205_171201.json
ğŸ’¾ Retrieved chunks saved to: output/retrievals/retrieval_20260205_171201.json

âœ… All validation checks passed!
```

## ğŸ“ Project Structure

```
agentic_rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py                      # Package initialization
â”‚   â”œâ”€â”€ config.py                        # âš™ï¸ Centralized configuration (paths, clients, settings)
â”‚   â”œâ”€â”€ state.py                         # ğŸ“Š State definitions (AgenticRAGState TypedDict)
â”‚   â”œâ”€â”€ memory.py                        # ğŸ’¾ Memory system (WorkflowMemory class)
â”‚   â”œâ”€â”€ workflow_simple.py               # ğŸ”„ Simple workflow (linear, no retry)
â”‚   â”œâ”€â”€ workflow.py                      # ğŸ”„ Full workflow (with retry logic)
â”‚   â”œâ”€â”€ agents_coordinator.py            # ğŸ­ Agent coordinator (facade pattern)
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/                          # ğŸ¤– Agent implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                      # Base agent class
â”‚   â”‚   â”œâ”€â”€ orchestrator.py              # Step 1: Question analysis
â”‚   â”‚   â”œâ”€â”€ query_planner.py             # Step 2: Query generation
â”‚   â”‚   â”œâ”€â”€ retrieval_router.py          # Step 3: Retrieval dispatcher
â”‚   â”‚   â”œâ”€â”€ retrieval_router_direct.py   # Direct mode (0 LLM calls)
â”‚   â”‚   â”œâ”€â”€ retrieval_router_planning.py # Planning mode (1 LLM call)
â”‚   â”‚   â”œâ”€â”€ retrieval_router_tool_calling.py # Tool calling mode (5-15 LLM calls)
â”‚   â”‚   â””â”€â”€ evidence_judge.py            # Step 4: Evidence assessment
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/                           # ğŸ”§ Retrieval and build tools
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ retrieval_tools.py           # Main retrieval tools (Range, BM25, Semantic, Hybrid)
â”‚   â”‚   â”œâ”€â”€ bm25_store.py                # BM25 index wrapper
â”‚   â”‚   â”œâ”€â”€ chroma_store.py              # ChromaDB wrapper
â”‚   â”‚   â”œâ”€â”€ build_indexes.py             # ğŸ“¦ Unified index builder (auto-check)
â”‚   â”‚   â”œâ”€â”€ build_range_index.py         # Range index builder
â”‚   â”‚   â”œâ”€â”€ build_bm25.py                # BM25 index builder
â”‚   â”‚   â””â”€â”€ build_embeddings_chroma.py   # ChromaDB embeddings builder
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/                         # ğŸ“ LLM prompt templates
â”‚   â”‚   â”œâ”€â”€ orchestrator.txt
â”‚   â”‚   â”œâ”€â”€ query_planner.txt
â”‚   â”‚   â”œâ”€â”€ evidence_judge.txt
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ utils/                           # ğŸ› ï¸ Utility functions
â”‚       â”œâ”€â”€ keyword_parser.py
â”‚       â””â”€â”€ save_retrieval.py            # Save retrieved chunks
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ build/                           # ğŸ—ï¸ Built indexes
â”‚   â”‚   â”œâ”€â”€ chunks.jsonl                 # Processed chunks (~400 chunks)
â”‚   â”‚   â”œâ”€â”€ pages.jsonl                  # Page metadata
â”‚   â”‚   â”œâ”€â”€ table_of_contents.json       # TOC structure
â”‚   â”‚   â”œâ”€â”€ cpt_range_index.db           # Range routing index (SQLite, 2.4MB)
â”‚   â”‚   â”œâ”€â”€ bm25_index.pkl               # BM25 index (Pickle)
â”‚   â”‚   â””â”€â”€ chroma_db/                   # ChromaDB vector store
â”‚   â”‚       â”œâ”€â”€ chroma.sqlite3           # Metadata (16MB, 481 embeddings)
â”‚   â”‚       â””â”€â”€ <uuid-dirs>/             # Vector segments
â”‚   â”‚
â”‚   â””â”€â”€ data/                            # ğŸ“„ Source data
â”‚       â”œâ”€â”€ raw/                         # Raw PDF files
â”‚       â””â”€â”€ processed/                   # Processed data
â”‚
â”œâ”€â”€ output/                              # ğŸ“¤ Output directory
â”‚   â”œâ”€â”€ queries/                         # Query execution logs
â”‚   â”œâ”€â”€ evaluations/                     # Evaluation results
â”‚   â””â”€â”€ retrievals/                      # ğŸ’¾ Retrieved chunks (JSON)
â”‚       â””â”€â”€ retrieval_20260205_171201.json
â”‚
â”œâ”€â”€ memory/                              # ğŸ’¾ Workflow execution history
â”‚   â”œâ”€â”€ workflow_simple_direct_*.json    # Direct mode executions
â”‚   â”œâ”€â”€ workflow_simple_planning_*.json  # Planning mode executions
â”‚   â”œâ”€â”€ workflow_simple_tool_calling_*.json  # Tool calling executions
â”‚   â”œâ”€â”€ latest_simple_direct.json        # Latest direct mode
â”‚   â”œâ”€â”€ latest_simple_planning.json      # Latest planning mode
â”‚   â””â”€â”€ latest_simple_tool_calling.json  # Latest tool calling
â”‚
â”œâ”€â”€ docs/                                # ğŸ“š Documentation
â”‚   â”œâ”€â”€ orchestrator_advantages_and_limitations.md
â”‚   â”œâ”€â”€ retrieval_router_design.md
â”‚   â”œâ”€â”€ retrieval_router_modes.md
â”‚   â”œâ”€â”€ retrieval_strategy_execution_modes.md
â”‚   â”œâ”€â”€ SIMPLE_WORKFLOW_ARCHITECTURE.md
â”‚   â””â”€â”€ tool_calling_patterns_comparison.md
â”‚
â”œâ”€â”€ testing/                             # ğŸ§ª Test files
â”‚   â”œâ”€â”€ test_retrieval_router_direct.py
â”‚   â”œâ”€â”€ test_retrieval_router_planning.py
â”‚   â””â”€â”€ test_retrieval_router_tool_calling.py
â”‚
â”œâ”€â”€ test_workflow_simple.py              # ğŸ§ª Main test script
â”œâ”€â”€ test_build_indexes.py                # ğŸ§ª Index building test
â”œâ”€â”€ requirements.txt                     # ğŸ“¦ Python dependencies
â”œâ”€â”€ .env                                 # ğŸ” Environment variables
â””â”€â”€ README.md                            # ğŸ“– This file
```

### Key Files

| File | Purpose |
|------|---------|
| `src/config.py` | Single source of truth for all paths and clients |
| `src/workflow_simple.py` | Main workflow orchestration |
| `src/memory.py` | Execution history management |
| `src/tools/build_indexes.py` | Auto-check and build all indexes |
| `test_workflow_simple.py` | Comprehensive workflow testing |

## ğŸ“š Usage Examples

### Python API Usage

```python
from src.workflow_simple import SimpleAgenticRAGWorkflow
from src.config import AgenticRAGConfig

# Load configuration
config = AgenticRAGConfig.from_env()

# Create workflow (auto-checks indexes on init)
workflow = SimpleAgenticRAGWorkflow(config, enable_memory=True)

# Execute workflow
result = workflow.run(
    question="What is CPT code 14301 and when can it be billed with 27702?",
    cpt_code=14301
)

# Access results
print(f"Retrieved {len(result['retrieved_chunks'])} chunks")
print(f"Evidence sufficient: {result['evidence_assessment']['is_sufficient']}")
print(f"Coverage score: {result['evidence_assessment']['coverage_score']}")
```

### Change Retrieval Mode

Edit `src/config.py`:

```python
class AgenticRAGConfig(BaseModel):
    retrieval_mode: str = "direct"  # or "planning" or "tool_calling"
```

Or set environment variable:

```bash
export RETRIEVAL_MODE=planning
python test_workflow_simple.py
```

### Access Memory

```python
from src.memory import WorkflowMemory

memory = WorkflowMemory(memory_dir="memory")

# Load latest execution
latest = memory.load_latest(workflow_type="simple")

print(f"Question: {latest['metadata']['question']}")
print(f"Mode: {latest['retrieval']['retrieval_metadata']['mode']}")
print(f"Chunks: {latest['retrieval']['num_chunks']}")

# List history
history = memory.list_history(workflow_type="simple", limit=10)
for item in history:
    print(f"{item['timestamp']}: {item['question'][:50]}...")
```

### Analyze Retrieved Chunks

```python
import json

# Load retrieved chunks
with open("output/retrievals/retrieval_20260205_171201.json") as f:
    data = json.load(f)

print(f"Question: {data['question']}")
print(f"Mode: {data['metadata']['mode']}")
print(f"Total chunks: {data['num_chunks']}")

# Analyze chunks
for i, chunk in enumerate(data['chunks'][:5], 1):
    print(f"\nChunk {i} (score: {chunk['score']:.2f}):")
    print(f"  ID: {chunk['chunk_id']}")
    print(f"  Text: {chunk['text'][:100]}...")
```

## ğŸ› ï¸ Advanced Configuration

### Retrieval Parameters

In `src/config.py`:

```python
# Retrieval settings
top_k: int = 15  # Final number of chunks to retrieve
retrieval_mode: str = "tool_calling"  # direct/planning/tool_calling

# Evidence judge thresholds
min_coverage_score: float = 0.7
min_specificity_score: float = 0.7
min_citation_count: int = 3

# Agent LLM settings
agent_temperature: float = 0
agent_max_tokens: int = 2000
```

### Build Index Settings

In `src/tools/build_embeddings_chroma.py`:

```python
BATCH_SIZE = 100  # Number of texts per batch
SLEEP_TIME = 0.5  # Sleep time between batches (seconds)
COLLECTION_NAME = "ncci_chunks"  # ChromaDB collection name
```

### Force Rebuild Indexes

```bash
# Rebuild all indexes
python -m src.tools.build_indexes --force

# Rebuild specific index
rm rag/build/bm25_index.pkl
python -m src.tools.build_indexes
```

## ğŸ“Š Performance Benchmarks

| Mode | LLM Calls | Avg Time | Avg Cost | Chunks Quality |
|------|-----------|----------|----------|----------------|
| Direct | 0 (retrieval) | 0.5s | $0 | â­â­â­ |
| Planning | 1 | 2-3s | $0.01 | â­â­â­â­ |
| Tool Calling | 5-15 | 8-12s | $0.05-0.15 | â­â­â­â­â­ |

*Note: All modes use LLM for orchestrator, query planner, and evidence judge*

## ğŸ” Detailed Execution Logs

### Memory File Structure

```json
{
  "metadata": {
    "timestamp": "2026-02-05T17:12:01",
    "question": "What is CPT code 14301...",
    "workflow_type": "simple",
    "success": true
  },
  "orchestrator": {
    "question_type": "PTP",
    "question_complexity": "medium",
    "retrieval_strategies": ["range_routing", "bm25", "semantic"]
  },
  "query_planner": {
    "num_candidates": 4,
    "query_candidates": [...]
  },
  "retrieval": {
    "num_chunks": 20,
    "retrieval_metadata": {
      "mode": "tool_calling",
      "execution_log": [
        {"iteration": 1, "tool_name": "range_routing", "chunks_returned": 50},
        {"iteration": 2, "tool_name": "bm25_search", "chunks_returned": 20},
        ...
      ],
      "saved_to": "output/retrievals/retrieval_20260205_171201.json"
    }
  },
  "evidence_judge": {
    "is_sufficient": false,
    "coverage_score": 0.30,
    "specificity_score": 0.30,
    "missing_aspects": [...]
  }
}
```

## ğŸ§ª Testing

```bash
# Run main test
python test_workflow_simple.py

# Test index building
python test_build_indexes.py

# Test specific retrieval mode
# (Edit config.py retrieval_mode first)
python test_workflow_simple.py
```

## ğŸ“– Documentation

- [Simple Workflow Architecture](docs/SIMPLE_WORKFLOW_ARCHITECTURE.md)
- [Retrieval Router Modes](docs/retrieval_router_modes.md)
- [Tool Calling Patterns](docs/tool_calling_patterns_comparison.md)
- [Orchestrator Design](docs/orchestrator_advantages_and_limitations.md)

## ğŸ“„ License

MIT License

## ğŸ“§ Contact

For questions or suggestions, please submit an Issue or contact the project maintainers.

---

**Built with â¤ï¸ using LangGraph and Azure OpenAI**
